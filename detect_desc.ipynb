{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "734e7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to data/extracted/cmp_yesbank.csv\n",
      "Total rows: 751\n",
      "Rows processed (containing transaction types): 655\n"
     ]
    }
   ],
   "source": [
    "# transaction types\n",
    "TRANSACTION_TYPES = {\n",
    "    'NEFT', 'RTGS', 'POS', 'ACH', 'IMPS', 'UPI',\n",
    "    'NACH', 'FT', 'DD', 'ECS', 'AEPS', 'SWIFT'\n",
    "}\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model 'en_core_web_lg'...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# pre-compile regex patterns\n",
    "ACCOUNT_NO_PATTERN = re.compile(r'([A-Z]{4,}\\d{6,}\\w*)')\n",
    "ALPHA_NUMERIC_WORD_PATTERN = re.compile(r'\\b(?=\\w*[A-Za-z])(?=\\w*\\d)\\w+\\b')\n",
    "TRANSACTION_TYPE_PATTERN = re.compile(r'\\b(?:' + '|'.join(TRANSACTION_TYPES) + r')\\b', re.IGNORECASE)\n",
    "NON_ALPHA_SPACE_PATTERN = re.compile(r'[^A-Za-z\\s]')\n",
    "MONTHS_REMOVE = re.compile(r'(?i)\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|sept?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\b')\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'CR', 'DR', 'BY', 'TO', 'FROM', 'TRANSFER', 'PAYMENT',\n",
    "    'CREDIT', 'DEBIT', 'THROUGH', 'VIA', 'TRANSACTION', 'CHQ', \n",
    "    'ADV', 'CHEQUE', 'DEPOSIT', 'OUTWARD', 'INWARD'\n",
    "}\n",
    "\n",
    "def should_process(text):\n",
    "    \"\"\"Check if text contains any of the transaction types\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return False\n",
    "\n",
    "    return any(trans_type in text.upper() for trans_type in TRANSACTION_TYPES)\n",
    "\n",
    "def extract_after_account_no(text):\n",
    "    \"\"\"\n",
    "    Detect account number and return everything after it.\n",
    "    \"\"\"\n",
    "    match = ACCOUNT_NO_PATTERN.search(text)\n",
    "    if match:\n",
    "        return text[match.end():].strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean bank statement description text using regex\"\"\"\n",
    "    if not should_process(text):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Extract everything after the account number\n",
    "    processed_text = extract_after_account_no(text)\n",
    "\n",
    "    # 2. Remove any group that contains both letters and digits (alphanumeric words)\n",
    "    processed_text = ALPHA_NUMERIC_WORD_PATTERN.sub('', processed_text)\n",
    "\n",
    "    # 3. Remove transaction type keywords (case-insensitive, as whole words)\n",
    "    processed_text = TRANSACTION_TYPE_PATTERN.sub('', processed_text)\n",
    "\n",
    "    # 4. Remove abbreviated months and dates\n",
    "    processed_text = MONTHS_REMOVE.sub('', processed_text)\n",
    "\n",
    "    # 5. Remove special characters except alphabets and spaces, convert to uppercase, and collapse spaces\n",
    "    processed_text = NON_ALPHA_SPACE_PATTERN.sub(' ', processed_text)\n",
    "    processed_text = processed_text.upper()\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "\n",
    "    # 6. Remove common transaction-related words\n",
    "    words = [w for w in processed_text.split() if w not in STOP_WORDS]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract entities from text using spaCy's built-in NER\"\"\"\n",
    "    if not text:  # Skip empty strings\n",
    "        return []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    orgs = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return orgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91172a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_file, output_file):\n",
    "    \"\"\"Process bank statements from CSV and save results\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_file}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file '{input_file}': {e}\")\n",
    "        return\n",
    "\n",
    "    df_processed = df.copy()\n",
    "    desc_column = df.columns[0]\n",
    "\n",
    "    rows_to_process_mask = df_processed[desc_column].apply(should_process)\n",
    "    df_processed.loc[rows_to_process_mask, 'Cleaned_Description'] = df_processed.loc[rows_to_process_mask, desc_column].apply(clean_text)\n",
    "\n",
    "    df_processed['Extracted_Entities'] = None\n",
    "\n",
    "    # Extract entities only for the relevant rows\n",
    "    if rows_to_process_mask.any():\n",
    "        df_processed.loc[rows_to_process_mask, 'Extracted_Entities'] = df_processed.loc[rows_to_process_mask, 'Cleaned_Description'].apply(extract_entities)\n",
    "\n",
    "    try:\n",
    "        df_processed.to_csv(output_file, index=False)\n",
    "        print(f\"Processed data saved to {output_file}\")\n",
    "        print(f\"Total rows: {len(df)}\")\n",
    "        print(f\"Rows processed (containing transaction types): {rows_to_process_mask.sum()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processed data to '{output_file}': {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/desc/yesbank_desc.csv\"  # input file\n",
    "    output_file = \"data/extracted/cmp_yesbank.csv\"\n",
    "\n",
    "    try:\n",
    "        process_csv(input_file, output_file)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Configuration error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44df003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Matches Found:\n",
      "{'john doe': {'end': 6, 'original': 'John Doe', 'start': 4},\n",
      " 'jon doe': {'end': 6, 'original': 'Jon Doe', 'start': 4},\n",
      " 'upi': {'end': 6,\n",
      "         'fuzzy_matches': [{'end': 8, 'start': 7, 'token': 'upi'}],\n",
      "         'original': 'UPI',\n",
      "         'start': 4}}\n"
     ]
    }
   ],
   "source": [
    "# clustering and fuzzy matching\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from rapidfuzz import fuzz\n",
    "from pprint import pprint  # Importing pprint for cleaner output\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initialize PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract named entities from text using spaCy's NER.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return {ent.text.lower(): ent.text for ent in doc.ents}\n",
    "\n",
    "def create_phrase_matcher(entities):\n",
    "    \"\"\"Create a PhraseMatcher for the given entities.\"\"\"\n",
    "    patterns = [nlp.make_doc(entity) for entity in entities]\n",
    "    matcher.add(\"EntityMatcher\", patterns)\n",
    "\n",
    "def find_fuzzy_matches(text, entities, threshold=80):\n",
    "    \"\"\"Find fuzzy matches of entities in the text.\"\"\"\n",
    "    matches = {}\n",
    "    for entity, original in entities.items():\n",
    "        # Find the first exact match\n",
    "        matcher.add(\"ExactMatch\", [nlp.make_doc(original)])\n",
    "        doc = nlp(text)\n",
    "        matches_found = matcher(doc)\n",
    "        if matches_found:\n",
    "            first_match = matches_found[0]\n",
    "            start, end = first_match[1], first_match[2]\n",
    "            matches[entity] = {'original': original, 'start': start, 'end': end}\n",
    "            # Find fuzzy matches\n",
    "            for i in range(end, len(doc)):\n",
    "                token = doc[i].text.lower()\n",
    "                if fuzz.ratio(entity, token) >= threshold:\n",
    "                    matches[entity].setdefault('fuzzy_matches', []).append({'token': token, 'start': i, 'end': i+1})\n",
    "    return matches\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Process text to find entities and their fuzzy matches.\"\"\"\n",
    "    entities = extract_entities(text)\n",
    "    create_phrase_matcher(entities)\n",
    "    matches = find_fuzzy_matches(text, entities)\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "text = \"I transferred money to John Doe via UPI. Later, Jon Doe received it.\"\n",
    "matches = process_text(text)\n",
    "\n",
    "# Pretty-print the matches\n",
    "print(\"Entity Matches Found:\")\n",
    "pprint(matches, width=80)  # Adjust width as needed for better readability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
