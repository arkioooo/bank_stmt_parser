{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ced13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted from text - Account No: 922030029810224, Account Holder: ADITYA ENTERPRISES\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# classifications dict\n",
    "CLASSIFICATIONS = {\n",
    "    'ECS/NACH': ['ecs', 'nach', 'ach', 'ift'],\n",
    "    'IMPS': ['imps'],\n",
    "    'NEFT': ['neft'],\n",
    "    'RTGS': ['rtgs'],\n",
    "    'ESIC' : ['esic'],\n",
    "    'Interest' : ['interest'],\n",
    "    'Refund/Reversal' : ['refund', 'reversal', 'rev'],\n",
    "    'Salary': ['salary', 'stipend'],\n",
    "    'Tax': ['tax', 'duty', 'customs'],\n",
    "    'Credit Card' : ['credit card','cc'],\n",
    "    'Debit Card' : ['debit card', 'dc'],\n",
    "    'Bank Instrument' : ['dd', 'commissioner'],\n",
    "    'Cash Txn' : ['cash', 'withdrawal', 'deposit'],\n",
    "    'Cheque Txn' : ['cheque', 'chq', 'clearing', 'clg'],\n",
    "    'Company Expense' : ['expense', 'business', 'corporate', 'travel', \n",
    "                         'home', 'charges', 'employee', 'fund', 'reimbursement', \n",
    "                         'renumeration', 'leave'],\n",
    "    'Forex' : ['forex', 'brn'],\n",
    "    'Insurance' : ['insurance', 'premium'],\n",
    "    'Rent' : ['rent'],\n",
    "    'UPI' : ['upi'],\n",
    "    'Other': []\n",
    "}\n",
    "\n",
    "desc_list = ['description', 'remark', 'transaction', 'detail', 'particulars']\n",
    "\n",
    "def extract_account_info_from_text(text, pdf_path=None, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Extract account number and account holder name from PDF text content.\n",
    "    If not found, use OCR on the provided bbox to extract account number.\n",
    "    \"\"\"\n",
    "    account_number = None\n",
    "    account_holder = None\n",
    "\n",
    "    # regex for acc no (allowing for alphanumeric, e.g. SBIN0001234567)\n",
    "    acc_no_patterns = [\n",
    "        r'Account\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'A/c\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'Account\\s*Number\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'Acc\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})'\n",
    "    ]\n",
    "\n",
    "    for pattern in acc_no_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            account_number = match.group(1)\n",
    "            break\n",
    "\n",
    "    # OCR fallback if not found\n",
    "    if not account_number and pdf_path:\n",
    "        ocr_text = ocr_text_from_bbox(pdf_path, poppler_bin, page_num)\n",
    "        # Try to extract account number from OCR text\n",
    "        for pattern in acc_no_patterns:\n",
    "            match = re.search(pattern, ocr_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                account_number = match.group(1)\n",
    "                break\n",
    "        # If still not found, try to find any long alphanumeric sequence\n",
    "        if not account_number:\n",
    "            generic_match = re.search(r'\\b[A-Z0-9]{6,}\\b', ocr_text)\n",
    "            if generic_match:\n",
    "                account_number = generic_match.group(0)\n",
    "\n",
    "    # Account holder extraction (same as your logic)\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines[:20]):\n",
    "        line = line.strip()\n",
    "        if not line or any(skip in line.lower() for skip in ['statement', 'account', 'period', 'bank', 'branch', 'customer']):\n",
    "            continue\n",
    "        if (line.isupper() and len(line.split()) <= 3 and len(line) > 5 and \n",
    "            not any(char.isdigit() for char in line) and\n",
    "            ('ENTERPRISES' in line or 'PRIVATE' in line or 'LIMITED' in line or 'PVT' in line or 'LTD' in line) or\n",
    "            (len(line.split()) == 2 and all(word.isalpha() for word in line.split()))):\n",
    "            account_holder = line\n",
    "            break\n",
    "\n",
    "    # look for patterns around \"Joint Holder\" or similar\n",
    "    joint_holder_match = re.search(r'([A-Z\\s]+)\\s*Joint\\s+Holder', text, re.IGNORECASE)\n",
    "    if joint_holder_match and not account_holder:\n",
    "        potential_name = joint_holder_match.group(1).strip()\n",
    "        if len(potential_name) > 3:\n",
    "            account_holder = potential_name\n",
    "\n",
    "    return account_number, account_holder\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract all text content from PDF for parsing account information.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "def ocr_page_to_dataframe(pdf_path, page_num, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Use OCR to extract tabular data from a PDF page when table detection fails.\n",
    "    \"\"\"\n",
    "    # print(f\"Using OCR for page {page_num + 1}...\")\n",
    "    images = convert_from_path(pdf_path, dpi=500, poppler_path=poppler_bin)\n",
    "\n",
    "    text1= ''\n",
    "    for i, page in enumerate(images):\n",
    "        text = pytesseract.image_to_string(page)\n",
    "        text1 += text\n",
    "        print(f\"\\n{'='*30}\")\n",
    "        print(f\"   OCR Text - Page {i+1}\")\n",
    "        print(f\"{'='*30}\\n\")\n",
    "        print(text.strip())\n",
    "        print(\"\\n\")\n",
    "    return parse_ocr_text_to_dataframe(text1)\n",
    "    \n",
    "\n",
    "def parse_ocr_text_to_dataframe(text):\n",
    "    \"\"\"\n",
    "    Parse OCR text into a structured dataframe for bank statements.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        if any(skip in line.lower() for skip in ['opening balance', 'closing balance', 'transaction total', 'statement', 'account', 'branch', 'address']):\n",
    "            continue\n",
    "        print(\"check1\")\n",
    "        date_match = re.search(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{4})\\b', line)\n",
    "        if date_match:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4: # min parts in transaction\n",
    "                try:\n",
    "                    date = date_match.group(1)\n",
    "                    amounts = []\n",
    "                    description_parts = []\n",
    "                    \n",
    "                    for part in parts:\n",
    "                        clean_part = part.replace(',', '').replace('(', '').replace(')', '')\n",
    "                        try:\n",
    "                            amount = float(clean_part)\n",
    "                            amounts.append(amount)\n",
    "                        except ValueError:\n",
    "                            if part != date:\n",
    "                                description_parts.append(part)\n",
    "                    \n",
    "                    if amounts:\n",
    "                        description = ' '.join(description_parts)\n",
    "                        debit = None\n",
    "                        credit = None\n",
    "                        balance = None\n",
    "                        \n",
    "                        if len(amounts) >= 3:\n",
    "                            balance = amounts[-1]  # last amount is usually balance\n",
    "                            if len(amounts) == 3:\n",
    "                                if amounts[0] != 0:\n",
    "                                    debit = amounts[0]\n",
    "                                if amounts[1] != 0:\n",
    "                                    credit = amounts[1]\n",
    "                        elif len(amounts) == 2:\n",
    "                            balance = amounts[-1]\n",
    "                            if 'cr' in line.lower() or 'credit' in line.lower():\n",
    "                                credit = amounts[0]\n",
    "                            else:\n",
    "                                debit = amounts[0]\n",
    "                        elif len(amounts) == 1:\n",
    "                            balance = amounts[0]\n",
    "                        \n",
    "                        rows.append({\n",
    "                            'date': date,\n",
    "                            'description': description,\n",
    "                            'debit': debit,\n",
    "                            'credit': credit,\n",
    "                            'balance': balance\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        # print(f\"OCR extracted {len(df)} potential transactions\")\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_tables(pdf_path, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Extract tables from PDF and return combined dataframe.\n",
    "    Uses OCR as fallback when table detection fails.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    dfs = []\n",
    "    \n",
    "    # print(f\"Processing {len(doc)} pages\")\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        # print(f\"Processing page {page_num + 1}...\")\n",
    "        tables = page.find_tables()\n",
    "        page_has_data = False\n",
    "        \n",
    "        # fitz table detection\n",
    "        for table_num, tbl in enumerate(tables):\n",
    "            try:\n",
    "                df = tbl.to_pandas()\n",
    "                if not df.empty:\n",
    "                    # print(f\"Found table {table_num + 1} with {len(df)} rows\")\n",
    "                    df = df.dropna(how='all')\n",
    "\n",
    "                    string_cols = df.select_dtypes(include=['object']).columns\n",
    "                    if len(string_cols) > 0:\n",
    "                        # Check if all string columns are empty/whitespace\n",
    "                        mask = df[string_cols].astype(str).apply(lambda x: x.str.strip()).replace('', pd.NA).notna().any(axis=1)\n",
    "                        df = df[mask]\n",
    "                    \n",
    "                    if not df.empty:\n",
    "                        dfs.append(df)\n",
    "                        # print(f\"Added {len(df)} valid rows from table {table_num + 1}\")\n",
    "                        page_has_data = True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing table {table_num + 1} on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # or try OCR as fallback\n",
    "        if not page_has_data:\n",
    "            images = convert_from_path(pdf_path, dpi=500, poppler_path=poppler_bin, first_page=page_num+1, last_page=page_num+1)\n",
    "            if images:\n",
    "                text = pytesseract.image_to_string(images[0])\n",
    "                # Save raw text in a DataFrame\n",
    "                raw_df = pd.DataFrame({'raw_text': [text], 'page_num': [page_num+1]})\n",
    "                dfs.append(raw_df)\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        # print(f\"Total combined rows: {len(combined_df)}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def ocr_text_from_bbox(pdf_path, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Crop the page image using bbox and perform OCR on cropped image.\n",
    "    bbox format: (left, upper, right, lower)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=500, poppler_path=poppler_bin)\n",
    "        if page_num >= len(images):\n",
    "            return \"\"\n",
    "        \n",
    "        # set tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(images).strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def classify_transaction(description):\n",
    "    \"\"\"\n",
    "    Classify transactions based on keywords in description.\n",
    "    \"\"\"\n",
    "    if pd.isna(description):\n",
    "        return 'Other'\n",
    "        \n",
    "    description_lower = str(description).lower()\n",
    "    \n",
    "    for category, keywords in CLASSIFICATIONS.items():\n",
    "        if any(keyword in description_lower for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "def normalize(df):\n",
    "    \"\"\"\n",
    "    Normalize and clean the extracted dataframe.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        columns = ['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    df.columns = (df.columns\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .str.replace(r'\\s+', '_', regex=True)\n",
    "                    .str.replace(r'\\.', '', regex=True))\n",
    "\n",
    "    # Find the first description-like column present\n",
    "    desc_cols_found = [col for col in desc_list if col in df.columns]\n",
    "    if desc_cols_found:\n",
    "        desc_col = desc_cols_found[0]\n",
    "        df.rename(columns={desc_col: 'description'}, inplace=True)\n",
    "    else:\n",
    "        df['description'] = pd.NA\n",
    "\n",
    "    date_columns = [col for col in df.columns if 'date' in col]\n",
    "    if date_columns:\n",
    "        df['date'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "        df['month_year'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%m-%Y')\n",
    "    else:\n",
    "        df['date'] = None\n",
    "        df['month_year'] = None\n",
    "\n",
    "    df['serial_no'] = range(1, len(df) + 1)\n",
    "\n",
    "    for col in ['debit', 'credit', 'balance']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').replace({'nan': None, 'NaN': None, '': None})\n",
    "            df[col] = df[col].apply(lambda x: pd.to_numeric(x, errors='coerce') if x is not None else pd.NA)\n",
    "        else:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # classification\n",
    "    df['classification'] = df['description'].apply(classify_transaction)\n",
    "    df['account_holders_name'] = None\n",
    "\n",
    "    return df[['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']]\n",
    "\n",
    "def process_statement(input_pdf, output_file, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process bank statement PDF and extract structured data.\n",
    "    Uses text extraction as primary method, OCR as fallback.\n",
    "    \"\"\"\n",
    "    # print(f\"Processing {input_pdf}\")\n",
    "    \n",
    "    # use fitz\n",
    "    pdf_text = extract_text_from_pdf(input_pdf)\n",
    "    acc_no, acc_name = extract_account_info_from_text(pdf_text, pdf_path=input_pdf, poppler_bin=poppler_bin)\n",
    "    print(f\"Extracted from text - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    # or fallback to OCR\n",
    "    if (not acc_no or not acc_name):\n",
    "        print(\"Falling back to OCR method\")\n",
    "        if not acc_no:\n",
    "            acc_no = ocr_text_from_bbox(input_pdf, poppler_bin)\n",
    "        if not acc_name:\n",
    "            acc_name = ocr_text_from_bbox(input_pdf, poppler_bin)\n",
    "        # print(f\"OCR results - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    df_raw = extract_tables(input_pdf, poppler_bin)\n",
    "    if not df_raw.empty and 'raw_text' in df_raw.columns:\n",
    "        df_raw.to_csv(output_file.replace('.xlsx', '_raw_ocr.csv'), index=False)\n",
    "    # print(df_raw.tail())\n",
    "    # print(f\"Extracted {len(df_raw)} raw rows from tables\")\n",
    "    \n",
    "    df = normalize(df_raw)\n",
    "    \n",
    "    if acc_name:\n",
    "        df['account_holders_name'] = acc_name\n",
    "    if acc_no:\n",
    "        df['acc_no'] = acc_no\n",
    "    \n",
    "    # save to Excel\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} processed rows to {output_file}\")\n",
    "    \n",
    "    return df, acc_no, acc_name\n",
    "\n",
    "def process_folder(input_folder, output_folder, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process all PDFs in input_folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "    # print(f\"Found {len(pdf_files)} PDF files in {input_folder}\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        input_pdf = os.path.join(input_folder, pdf_file)\n",
    "        output_file = os.path.join(output_folder, os.path.splitext(pdf_file)[0] + \".xlsx\")\n",
    "        # print(f\"\\n Processing {input_pdf}\")\n",
    "        try:\n",
    "            df, account_number, account_holder = process_statement(input_pdf, output_file, poppler_bin)\n",
    "            # print(f\"Processed {output_file}, ({len(df)}) rows\")\n",
    "        except Exception as e:\n",
    "            # print(f\"Error processing {input_pdf}: {e}\")\n",
    "            continue\n",
    "\n",
    "def longest_common_prefix(strings):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    s1 = min(strings)\n",
    "    s2 = max(strings)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1\n",
    "\n",
    "def combine_excels_if_similar(output_folder, threshold=80):\n",
    "    \"\"\"\n",
    "    Combine Excel files in output_folder into a single Excel file if their filenames are at least\n",
    "    `threshold` percent similar (fuzzy match). Do not save individuals if combined.\n",
    "    \"\"\"\n",
    "    excel_files = [f for f in os.listdir(output_folder) if f.lower().endswith('.xlsx')]\n",
    "    if not excel_files:\n",
    "        # print(\"No Excel files found to combine.\")\n",
    "        return\n",
    "\n",
    "    groups = []\n",
    "    used = set()\n",
    "    for i, file1 in enumerate(excel_files):\n",
    "        if file1 in used:\n",
    "            continue\n",
    "        group = [file1]\n",
    "        used.add(file1)\n",
    "        for file2 in excel_files[i+1:]:\n",
    "            if file2 in used:\n",
    "                continue\n",
    "            score = fuzz.ratio(os.path.splitext(file1)[0], os.path.splitext(file2)[0])\n",
    "            if score >= threshold:\n",
    "                group.append(file2)\n",
    "                used.add(file2)\n",
    "        groups.append(group)\n",
    "\n",
    "    for group in groups:\n",
    "        if len(group) > 1:\n",
    "            # print(f\"Combining files: {group}\")\n",
    "            dfs = []\n",
    "            for fname in group:\n",
    "                df = pd.read_excel(os.path.join(output_folder, fname))\n",
    "                df['source_file'] = fname\n",
    "                dfs.append(df)\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "            # Remove individual files\n",
    "            for fname in group:\n",
    "                os.remove(os.path.join(output_folder, fname))\n",
    "            # Name combined file based on common prefix or joined names\n",
    "            base_names = [os.path.splitext(f)[0] for f in group]\n",
    "            prefix = longest_common_prefix(base_names).rstrip(\"_- \")\n",
    "            if not prefix or len(prefix) < 3:\n",
    "                prefix = \"_\".join(base_names)\n",
    "            combined_path = os.path.join(output_folder, f\"{prefix}_combined.xlsx\")\n",
    "            combined_df.to_excel(combined_path, index=False)\n",
    "            # print(f\"Combined Excel saved to {combined_path}\")\n",
    "        # else:\n",
    "            # print(f\"File {group[0]} has no similar files (>= {threshold}%) to combine. Keeping as is.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"data/old pdfs/\"\n",
    "    output_folder = r\"parsed_excels\" \n",
    "    poppler_bin = None\n",
    "\n",
    "    process_folder(input_folder, output_folder, poppler_bin)\n",
    "    combine_excels_if_similar(output_folder, threshold=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec2d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_name = \"kotak_parag\"\n",
    "pdf_path = \"data/old pdfs/kotak_parag.pdf\"\n",
    "poppler_bin = r\"D:\\\\Mridul.Intern\\\\poppler-23.05.0\\\\Library\\\\bin\"\n",
    "\n",
    "df = extract_tables(pdf_path, poppler_bin=poppler_bin)\n",
    "if not df.empty:\n",
    "    df.to_csv(\"ocr_kotak_parag.csv\", index=False)\n",
    "    print(\"Saved\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
