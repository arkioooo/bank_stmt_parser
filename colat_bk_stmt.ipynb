{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd669821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic table extraction using fitz\n",
    "\n",
    "# import fitz  # PyMuPDF\n",
    "# import pandas as pd\n",
    "\n",
    "# def extract_all_tables(pdf_path, csv_path):\n",
    "#     tables = []\n",
    "#     doc = fitz.open(pdf_path)\n",
    "\n",
    "#     for page_number, page in enumerate(doc, 1):\n",
    "#         table_data = page.find_tables()\n",
    "#         if table_data.tables:\n",
    "#             for table in table_data.tables:\n",
    "#                 raw_data = table.extract()\n",
    "#                 if raw_data and len(raw_data):\n",
    "#                     df = pd.DataFrame(raw_data)\n",
    "#                     tables.append(df)\n",
    "    \n",
    "#     if not tables:\n",
    "#         print(\"No tables found in the PDF.\")\n",
    "#         return None\n",
    "    \n",
    "#     result_df = pd.concat(tables, ignore_index=True)\n",
    "#     result_df = result_df.dropna(how='all')  # drop empty rows\n",
    "#     result_df.to_csv(csv_path, index=False)\n",
    "#     print(f\"Extracted {len(tables)} tables from the PDF and saved to {csv_path}.\")\n",
    "#     return result_df\n",
    "\n",
    "# # Example usage\n",
    "# pdf_path = r\"yesbank.pdf\"\n",
    "# csv_path = r\"test.csv\"\n",
    "\n",
    "# all_tables = extract_all_tables(pdf_path, csv_path)\n",
    "\n",
    "# if all_tables is not None:\n",
    "#     print(all_tables.head())\n",
    "#     print(\"\\nTotal rows extracted:\", len(all_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial no, [acc no], date, month-year, particulars, db/cr, balance, classification\n",
    "# serial no : counter for each row\n",
    "# acc no : use ocr\n",
    "# date : in table\n",
    "# month-year : derive from date\n",
    "# particulars : description\n",
    "# db/cr : in table\n",
    "# balance : in table\n",
    "# classification : derive from description [ECS/NACH, IMPS, NEFT, RTGS, Salary, Tax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58955222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files in data/old pdfs/\n",
      "\n",
      " Processing data/old pdfs/axis_aditya.pdf\n",
      "Processing data/old pdfs/axis_aditya.pdf\n",
      "Extracted from text - Account No: 922030029810224, Account Holder: ADITYA ENTERPRISES\n",
      "Processing 4 pages\n",
      "No valid tables found on page 4, trying OCR\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Total combined rows: 83\n",
      "   Tran Date Chq No Particulars Debit Credit Balance Init.\\nBr 30-09-2022  \\\n",
      "78       NaN    NaN         NaN   NaN    NaN     NaN       NaN        NaN   \n",
      "79       NaN    NaN         NaN   NaN    NaN     NaN       NaN        NaN   \n",
      "80       NaN    NaN         NaN   NaN    NaN     NaN       NaN        NaN   \n",
      "81       NaN    NaN         NaN   NaN    NaN     NaN       NaN        NaN   \n",
      "82       NaN    NaN         NaN   NaN    NaN     NaN       NaN        NaN   \n",
      "\n",
      "   Col1 922030029810224:Int.Coll:01-09-2022 to 30-09-\\n2022  ... 31-05-2023  \\\n",
      "78  NaN                                                NaN   ...        NaN   \n",
      "79  NaN                                                NaN   ...        NaN   \n",
      "80  NaN                                                NaN   ...        NaN   \n",
      "81  NaN                                                NaN   ...        NaN   \n",
      "82  NaN                                                NaN   ...        NaN   \n",
      "\n",
      "   922030029810224:Int.Coll:01-05-2023 to 31-05-\\n2023 20585.00 -1855316.00  \\\n",
      "78                                                NaN       NaN         NaN   \n",
      "79                                                NaN       NaN         NaN   \n",
      "80                                                NaN       NaN         NaN   \n",
      "81                                                NaN       NaN         NaN   \n",
      "82                                                NaN       NaN         NaN   \n",
      "\n",
      "   Sr. No.   Period         Recover Date              Charge Type Total(RS).  \\\n",
      "78       8  08-2022  2022-09-17 00:00:00  Monthly Service\\nCharge         25   \n",
      "79       9  09-2022  2022-10-22 00:00:00  Monthly Service\\nCharge         25   \n",
      "80      10  10-2022  2022-11-23 00:00:00  Monthly Service\\nCharge         25   \n",
      "81      11  11-2022  2022-12-16 00:00:00  Monthly Service\\nCharge         25   \n",
      "82      12  12-2022  2023-01-14 00:00:00  Monthly Service\\nCharge         25   \n",
      "\n",
      "   Charges(RS).  \n",
      "78           25  \n",
      "79           25  \n",
      "80           25  \n",
      "81           25  \n",
      "82           25  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Extracted 83 raw rows from tables\n",
      "Saved 83 processed rows to parsed_excels\\axis_aditya.xlsx\n",
      "Processed parsed_excels\\axis_aditya.xlsx, (83) rows\n",
      "File axis_aditya.xlsx has no similar files (>= 80%) to combine. Keeping as is.\n",
      "File canara_aditya.xlsx has no similar files (>= 80%) to combine. Keeping as is.\n",
      "File labhanshi_multitrade_pvt_ltd_cosmos_cc_labhanshi_multitrade_pvt_ltd_bank_statement_combined.xlsx has no similar files (>= 80%) to combine. Keeping as is.\n",
      "File labhanshi_multitrade_pvt_ltd_c_combined.xlsx has no similar files (>= 80%) to combine. Keeping as is.\n",
      "File lee_italia_ceramic_combined.xlsx has no similar files (>= 80%) to combine. Keeping as is.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mridul.intern\\AppData\\Local\\Temp\\ipykernel_18160\\2100753806.py:371: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['account_holders_name'] = acc_name\n",
      "C:\\Users\\mridul.intern\\AppData\\Local\\Temp\\ipykernel_18160\\2100753806.py:373: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['acc_no'] = acc_no\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# classifications dict\n",
    "CLASSIFICATIONS = {\n",
    "    'ECS/NACH': ['ecs', 'nach', 'ach', 'ift'],\n",
    "    'IMPS': ['imps'],\n",
    "    'NEFT': ['neft'],\n",
    "    'RTGS': ['rtgs'],\n",
    "    'ESIC' : ['esic'],\n",
    "    'Interest' : ['interest'],\n",
    "    'Refund/Reversal' : ['refund', 'reversal', 'rev'],\n",
    "    'Salary': ['salary', 'stipend'],\n",
    "    'Tax': ['tax', 'duty', 'customs'],\n",
    "    'Credit Card' : ['credit card','cc'],\n",
    "    'Debit Card' : ['debit card', 'dc'],\n",
    "    'Bank Instrument' : ['dd', 'commissioner'],\n",
    "    'Cash Txn' : ['cash', 'withdrawal', 'deposit'],\n",
    "    'Cheque Txn' : ['cheque', 'chq', 'clearing', 'clg'],\n",
    "    'Company Expense' : ['expense', 'business', 'corporate', 'travel', \n",
    "                         'home', 'charges', 'employee', 'fund', 'reimbursement', \n",
    "                         'renumeration', 'leave'],\n",
    "    'Forex' : ['forex', 'brn'],\n",
    "    'Insurance' : ['insurance', 'premium'],\n",
    "    'Rent' : ['rent'],\n",
    "    'UPI' : ['upi'],\n",
    "    'Other': []\n",
    "}\n",
    "\n",
    "desc_list = ['description', 'remark', 'transaction', 'detail', 'particulars']\n",
    "\n",
    "def extract_account_info_from_text(text, pdf_path=None, bbox_acc_no=None, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Extract account number and account holder name from PDF text content.\n",
    "    If not found, use OCR on the provided bbox to extract account number.\n",
    "    \"\"\"\n",
    "    account_number = None\n",
    "    account_holder = None\n",
    "\n",
    "    # regex for acc no (allowing for alphanumeric, e.g. SBIN0001234567)\n",
    "    acc_no_patterns = [\n",
    "        r'Account\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'A/c\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'Account\\s*Number\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'Acc\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})'\n",
    "    ]\n",
    "\n",
    "    for pattern in acc_no_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            account_number = match.group(1)\n",
    "            break\n",
    "\n",
    "    # OCR fallback if not found\n",
    "    if not account_number and pdf_path and bbox_acc_no:\n",
    "        ocr_text = ocr_text_from_bbox(pdf_path, bbox_acc_no, poppler_bin, page_num)\n",
    "        # Try to extract account number from OCR text\n",
    "        for pattern in acc_no_patterns:\n",
    "            match = re.search(pattern, ocr_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                account_number = match.group(1)\n",
    "                break\n",
    "        # If still not found, try to find any long alphanumeric sequence\n",
    "        if not account_number:\n",
    "            generic_match = re.search(r'\\b[A-Z0-9]{6,}\\b', ocr_text)\n",
    "            if generic_match:\n",
    "                account_number = generic_match.group(0)\n",
    "\n",
    "    # Account holder extraction (same as your logic)\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines[:20]):\n",
    "        line = line.strip()\n",
    "        if not line or any(skip in line.lower() for skip in ['statement', 'account', 'period', 'bank', 'branch', 'customer']):\n",
    "            continue\n",
    "        if (line.isupper() and len(line.split()) <= 3 and len(line) > 5 and \n",
    "            not any(char.isdigit() for char in line) and\n",
    "            ('ENTERPRISES' in line or 'PRIVATE' in line or 'LIMITED' in line or 'PVT' in line or 'LTD' in line) or\n",
    "            (len(line.split()) == 2 and all(word.isalpha() for word in line.split()))):\n",
    "            account_holder = line\n",
    "            break\n",
    "\n",
    "    # look for patterns around \"Joint Holder\" or similar\n",
    "    joint_holder_match = re.search(r'([A-Z\\s]+)\\s*Joint\\s+Holder', text, re.IGNORECASE)\n",
    "    if joint_holder_match and not account_holder:\n",
    "        potential_name = joint_holder_match.group(1).strip()\n",
    "        if len(potential_name) > 3:\n",
    "            account_holder = potential_name\n",
    "\n",
    "    return account_number, account_holder\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract all text content from PDF for parsing account information.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "def ocr_page_to_dataframe(pdf_path, page_num, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Use OCR to extract tabular data from a PDF page when table detection fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"Using OCR for page {page_num + 1}...\")\n",
    "        images = convert_from_path(pdf_path, dpi=300, poppler_path=poppler_bin, first_page=page_num+1, last_page=page_num+1)\n",
    "        \n",
    "        if not images:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        img = images[0]\n",
    "        \n",
    "        # tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(img, config='--psm 6')\n",
    "        return parse_ocr_text_to_dataframe(text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(f\"OCR failed for page {page_num + 1}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def parse_ocr_text_to_dataframe(text):\n",
    "    \"\"\"\n",
    "    Parse OCR text into a structured dataframe for bank statements.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        if any(skip in line.lower() for skip in ['opening balance', 'closing balance', 'transaction total', 'statement', 'account', 'branch', 'address']):\n",
    "            continue\n",
    "        \n",
    "        date_match = re.search(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{4})\\b', line)\n",
    "        if date_match:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4: # min parts in transaction\n",
    "                try:\n",
    "                    date = date_match.group(1)\n",
    "                    amounts = []\n",
    "                    description_parts = []\n",
    "                    \n",
    "                    for part in parts:\n",
    "                        clean_part = part.replace(',', '').replace('(', '').replace(')', '')\n",
    "                        try:\n",
    "                            amount = float(clean_part)\n",
    "                            amounts.append(amount)\n",
    "                        except ValueError:\n",
    "                            if part != date:\n",
    "                                description_parts.append(part)\n",
    "                    \n",
    "                    if amounts:\n",
    "                        description = ' '.join(description_parts)\n",
    "                        debit = None\n",
    "                        credit = None\n",
    "                        balance = None\n",
    "                        \n",
    "                        if len(amounts) >= 3:\n",
    "                            balance = amounts[-1]  # last amount is usually balance\n",
    "                            if len(amounts) == 3:\n",
    "                                if amounts[0] != 0:\n",
    "                                    debit = amounts[0]\n",
    "                                if amounts[1] != 0:\n",
    "                                    credit = amounts[1]\n",
    "                        elif len(amounts) == 2:\n",
    "                            balance = amounts[-1]\n",
    "                            if 'cr' in line.lower() or 'credit' in line.lower():\n",
    "                                credit = amounts[0]\n",
    "                            else:\n",
    "                                debit = amounts[0]\n",
    "                        elif len(amounts) == 1:\n",
    "                            balance = amounts[0]\n",
    "                        \n",
    "                        rows.append({\n",
    "                            'date': date,\n",
    "                            'description': description,\n",
    "                            'debit': debit,\n",
    "                            'credit': credit,\n",
    "                            'balance': balance\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        # print(f\"OCR extracted {len(df)} potential transactions\")\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_tables(pdf_path, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Extract tables from PDF and return combined dataframe.\n",
    "    Uses OCR as fallback when table detection fails.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    dfs = []\n",
    "    \n",
    "    print(f\"Processing {len(doc)} pages\")\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        # print(f\"Processing page {page_num + 1}...\")\n",
    "        tables = page.find_tables()\n",
    "        page_has_data = False\n",
    "        \n",
    "        # fitz table detection\n",
    "        for table_num, tbl in enumerate(tables):\n",
    "            try:\n",
    "                df = tbl.to_pandas()\n",
    "                if not df.empty:\n",
    "                    # print(f\"Found table {table_num + 1} with {len(df)} rows\")\n",
    "                    df = df.dropna(how='all')\n",
    "\n",
    "                    string_cols = df.select_dtypes(include=['object']).columns\n",
    "                    if len(string_cols) > 0:\n",
    "                        # Check if all string columns are empty/whitespace\n",
    "                        mask = df[string_cols].astype(str).apply(lambda x: x.str.strip()).replace('', pd.NA).notna().any(axis=1)\n",
    "                        df = df[mask]\n",
    "                    \n",
    "                    if not df.empty:\n",
    "                        dfs.append(df)\n",
    "                        # print(f\"Added {len(df)} valid rows from table {table_num + 1}\")\n",
    "                        page_has_data = True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing table {table_num + 1} on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # or try OCR as fallback\n",
    "        if not page_has_data:\n",
    "            print(f\"No valid tables found on page {page_num + 1}, trying OCR\")\n",
    "            ocr_df = ocr_page_to_dataframe(pdf_path, page_num, poppler_bin)\n",
    "            if not ocr_df.empty:\n",
    "                dfs.append(ocr_df)\n",
    "                print(f\"OCR added {len(ocr_df)} rows from page {page_num + 1}\")\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Total combined rows: {len(combined_df)}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def ocr_text_from_bbox(pdf_path, bbox, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Crop the page image using bbox and perform OCR on cropped image.\n",
    "    bbox format: (left, upper, right, lower)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300, poppler_path=poppler_bin)\n",
    "        if page_num >= len(images):\n",
    "            return \"\"\n",
    "            \n",
    "        img = images[page_num].crop()\n",
    "        \n",
    "        # set tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(img, config='--psm 6').strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def classify_transaction(description):\n",
    "    \"\"\"\n",
    "    Classify transactions based on keywords in description.\n",
    "    \"\"\"\n",
    "    if pd.isna(description):\n",
    "        return 'Other'\n",
    "        \n",
    "    description_lower = str(description).lower()\n",
    "    \n",
    "    for category, keywords in CLASSIFICATIONS.items():\n",
    "        if any(keyword in description_lower for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "def normalize(df):\n",
    "    \"\"\"\n",
    "    Normalize and clean the extracted dataframe.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        columns = ['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    df.columns = (df.columns\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .str.replace(r'\\s+', '_', regex=True)\n",
    "                    .str.replace(r'\\.', '', regex=True))\n",
    "\n",
    "    # Find the first description-like column present\n",
    "    desc_cols_found = [col for col in desc_list if col in df.columns]\n",
    "    if desc_cols_found:\n",
    "        desc_col = desc_cols_found[0]\n",
    "        df.rename(columns={desc_col: 'description'}, inplace=True)\n",
    "    else:\n",
    "        df['description'] = pd.NA\n",
    "\n",
    "    date_columns = [col for col in df.columns if 'date' in col]\n",
    "    if date_columns:\n",
    "        df['date'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "        df['month_year'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%m-%Y')\n",
    "    else:\n",
    "        df['date'] = None\n",
    "        df['month_year'] = None\n",
    "\n",
    "    df['serial_no'] = range(1, len(df) + 1)\n",
    "\n",
    "    for col in ['debit', 'credit', 'balance']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').replace({'nan': None, 'NaN': None, '': None})\n",
    "            df[col] = df[col].apply(lambda x: pd.to_numeric(x, errors='coerce') if x is not None else pd.NA)\n",
    "        else:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # classification\n",
    "    df['classification'] = df['description'].apply(classify_transaction)\n",
    "    df['account_holders_name'] = None\n",
    "\n",
    "    return df[['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']]\n",
    "\n",
    "def process_statement(input_pdf, output_file, bbox_acc_no=None, bbox_acc_name=None, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process bank statement PDF and extract structured data.\n",
    "    Uses text extraction as primary method, OCR as fallback.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {input_pdf}\")\n",
    "    \n",
    "    # use fitz\n",
    "    pdf_text = extract_text_from_pdf(input_pdf)\n",
    "    acc_no, acc_name = extract_account_info_from_text(pdf_text, pdf_path=input_pdf, bbox_acc_no=bbox_acc_no, poppler_bin=poppler_bin)\n",
    "    print(f\"Extracted from text - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    # or fallback to OCR\n",
    "    if (not acc_no or not acc_name) and bbox_acc_no and bbox_acc_name:\n",
    "        print(\"Falling back to OCR method\")\n",
    "        if not acc_no:\n",
    "            acc_no = ocr_text_from_bbox(input_pdf, bbox_acc_no, poppler_bin)\n",
    "        if not acc_name:\n",
    "            acc_name = ocr_text_from_bbox(input_pdf, bbox_acc_name, poppler_bin)\n",
    "        # print(f\"OCR results - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    df_raw = extract_tables(input_pdf, poppler_bin)\n",
    "    print(df_raw.tail())\n",
    "    print(f\"Extracted {len(df_raw)} raw rows from tables\")\n",
    "    \n",
    "    df = normalize(df_raw)\n",
    "    \n",
    "    if acc_name:\n",
    "        df['account_holders_name'] = acc_name\n",
    "    if acc_no:\n",
    "        df['acc_no'] = acc_no\n",
    "    \n",
    "    # save to Excel\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} processed rows to {output_file}\")\n",
    "    \n",
    "    return df, acc_no, acc_name\n",
    "\n",
    "def process_folder(input_folder, output_folder, bbox_acc_no=None, bbox_acc_name=None, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process all PDFs in input_folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {input_folder}\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        input_pdf = os.path.join(input_folder, pdf_file)\n",
    "        output_file = os.path.join(output_folder, os.path.splitext(pdf_file)[0] + \".xlsx\")\n",
    "        print(f\"\\n Processing {input_pdf}\")\n",
    "        try:\n",
    "            df, account_number, account_holder = process_statement(input_pdf, output_file, bbox_acc_no, bbox_acc_name, poppler_bin)\n",
    "            print(f\"Processed {output_file}, ({len(df)}) rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_pdf}: {e}\")\n",
    "            continue\n",
    "\n",
    "def longest_common_prefix(strings):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    s1 = min(strings)\n",
    "    s2 = max(strings)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1\n",
    "\n",
    "def combine_excels_if_similar(output_folder, threshold=80):\n",
    "    \"\"\"\n",
    "    Combine Excel files in output_folder into a single Excel file if their filenames are at least\n",
    "    `threshold` percent similar (fuzzy match). Do not save individuals if combined.\n",
    "    \"\"\"\n",
    "    excel_files = [f for f in os.listdir(output_folder) if f.lower().endswith('.xlsx')]\n",
    "    if not excel_files:\n",
    "        print(\"No Excel files found to combine.\")\n",
    "        return\n",
    "\n",
    "    groups = []\n",
    "    used = set()\n",
    "    for i, file1 in enumerate(excel_files):\n",
    "        if file1 in used:\n",
    "            continue\n",
    "        group = [file1]\n",
    "        used.add(file1)\n",
    "        for file2 in excel_files[i+1:]:\n",
    "            if file2 in used:\n",
    "                continue\n",
    "            score = fuzz.ratio(os.path.splitext(file1)[0], os.path.splitext(file2)[0])\n",
    "            if score >= threshold:\n",
    "                group.append(file2)\n",
    "                used.add(file2)\n",
    "        groups.append(group)\n",
    "\n",
    "    for group in groups:\n",
    "        if len(group) > 1:\n",
    "            print(f\"Combining files: {group}\")\n",
    "            dfs = []\n",
    "            for fname in group:\n",
    "                df = pd.read_excel(os.path.join(output_folder, fname))\n",
    "                df['source_file'] = fname\n",
    "                dfs.append(df)\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "            # Remove individual files\n",
    "            for fname in group:\n",
    "                os.remove(os.path.join(output_folder, fname))\n",
    "            # Name combined file based on common prefix or joined names\n",
    "            base_names = [os.path.splitext(f)[0] for f in group]\n",
    "            prefix = longest_common_prefix(base_names).rstrip(\"_- \")\n",
    "            if not prefix or len(prefix) < 3:\n",
    "                prefix = \"_\".join(base_names)\n",
    "            combined_path = os.path.join(output_folder, f\"{prefix}_combined.xlsx\")\n",
    "            combined_df.to_excel(combined_path, index=False)\n",
    "            print(f\"Combined Excel saved to {combined_path}\")\n",
    "        else:\n",
    "            print(f\"File {group[0]} has no similar files (>= {threshold}%) to combine. Keeping as is.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"data/old pdfs/\"\n",
    "    output_folder = r\"parsed_excels\" \n",
    "    bbox_acc_no = (50, 100, 300, 150)\n",
    "    bbox_acc_name = (50, 50, 400, 100)\n",
    "    poppler_bin = None\n",
    "\n",
    "    process_folder(input_folder, output_folder, bbox_acc_no, bbox_acc_name, poppler_bin)\n",
    "    combine_excels_if_similar(output_folder, threshold=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
