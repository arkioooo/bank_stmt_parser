{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf187d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELIANCE DIGITAL → ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    " \n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "text = \"RELIANCE DIGITAL\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} → {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bb16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "   OCR Text - Page 1\n",
      "==============================\n",
      "\n",
      "AXIS BANK\n",
      "he\n",
      "\n",
      "ADITYA ENTERPRISES\n",
      "\n",
      "Joint Holder :- -\n",
      "\n",
      "DOOR NO 43 106 1 48\n",
      "\n",
      "8TH LANE THOTAVARI STREET\n",
      "\n",
      "NANDAMURI NAGAR\n",
      "\n",
      "KRISHNA\n",
      "\n",
      "ANDHRA PRADESH-INDIA\n",
      "\n",
      "520015\n",
      "\n",
      "Registered Mobile No :XXXXXX2506\n",
      "Registered Email ID:\n",
      "Scheme :SBB CGTMSE LIMIT\n",
      "\n",
      "Customer ID :877987892\n",
      "IFSC Code :UTIB0000555\n",
      "MICR Code :520211003\n",
      "Nominee Registered : N\n",
      "\n",
      "PAN :ALEPM4868K\n",
      "\n",
      "Statement of Axis Account No :922030029810224 for the period (From : 25-06-2022 To : 25-06-2023)\n",
      "\n",
      "Tran Date | Chq No Particulars Debit Credit Balance Init.\n",
      "Br\n",
      "\n",
      "OPENING BALANCE -42709.00\n",
      "9220300298 10224: Int.Coll:07-06-2022 to 30-06-\n",
      "\n",
      "30-06-2022 2022 293.00 -43002.00| 555\n",
      "BRN-CLG-CHQ PAID TO ADITYA\n",
      "\n",
      "01-07-2022 ENTERPRI/CANARA BANK 400000.00 -443002.00| 2568\n",
      "UPI/P2A/219200485097/M VENKATE/HDFC\n",
      "\n",
      "11-07-2022 BANK /interes 300.00 -442702.00| 555\n",
      "NEFT/P203220181099434/ADITYA\n",
      "\n",
      "22-07-2022 ENTERPRISES/Fundtran 200000.00 -242702.00| 248\n",
      "\n",
      "23-07-2022 GST @18% on Charge 4.50 -242706.50|_ 555\n",
      "\n",
      "23-07-2022 Consolidated Charges for A/c 25.00 -242731.50| 555\n",
      "9220300298 10224: Int.Coll:01-07-2022 to 31-07-\n",
      "\n",
      "31-07-2022 2022 3695.00 -246426.50| 555\n",
      "INB/NEFT/AX1C222135850968/ADITYA\n",
      "\n",
      "01-08-2022 EN/funds transfe 250000.00 -496426.50| 555\n",
      "IMPS/P2A/221319464935//CANARAB/X001631/\n",
      "\n",
      "01-08-2022 fund 250011.80 -746438.30| 555\n",
      "NEFT/P221220184506407/ADITYA\n",
      "\n",
      "09-08-2022 ENTERPRISES/FundTran 100000.00 -646438.30| 248\n",
      "BRN-CLG-CHQ PAID TO SHART SHIP\n",
      "\n",
      "19-08-2022 INNO/ICICI BANKING 100000.00 -746438.30| 2568\n",
      "IMPS/P2A/22361275 1249//CANARAB/X001631/\n",
      "\n",
      "24-08-2022 fund 250011.80 -996450.10| 555\n",
      "UPI/P2A/223946993226/A SOMASUN/Indian\n",
      "\n",
      "27-08-2022 Ov/Payment 25000.00 -971450.10| 555\n",
      "\n",
      "29-08-2022 GST @18% on Charge 4.50 -971454.60| 555\n",
      "\n",
      "29-08-2022 Consolidated Charges for A/c 25.00 -971479.60| 555\n",
      "9220300298 10224: Int.Coll:0 1-08-2022 to 31-08-\n",
      "\n",
      "31-08-2022 2022 7760.00 -979239.60| 555\n",
      "INB/NEFT/A X1C222449476477/ADITYA\n",
      "\n",
      "01-09-2022 ENTE/fund tranfe 150000.00 -1129239.60| 555\n",
      "INB/NEFT/A X1C222470760481/ADITYA\n",
      "\n",
      "04-09-2022 EN/funds transfe 50000.00 -1179239.60| 555\n",
      "NEFT/N248220155093815/RACE\n",
      "\n",
      "05-09-2022 ELECTRONICS AN 86900.00 -1092339.60| 248\n",
      "INB/NEFT/AX1C222502134926/AITVA HOME\n",
      "\n",
      "07-09-2022 APPL/paymen 100000.00 -1192339.60| 555\n",
      "INB/NEFT/A X1C2225 12647208/Securecom\n",
      "\n",
      "08-09-2022 IT/payment LF 156160.00 -1348499.60| 555\n",
      "INB/NEFT/A X1C222575214273/ADITYA\n",
      "\n",
      "14-09-2022 ENTERP/with draw 425000.00 -1773499.60| 555\n",
      "\n",
      "17-09-2022 GST @18% on Charge 4.50 -1773504.10| 555\n",
      "\n",
      "17-09-2022 Consolidated Charges for A/c 25.00 -1773529.10| 555\n",
      "UPI/P2A/226619989818/A SOMASUN/Indian\n",
      "\n",
      "23-09-2022 Ov/Payment 10000.00 -1763529.10| 555\n",
      "\n",
      "\n",
      "\n",
      "==============================\n",
      "   OCR Text - Page 2\n",
      "==============================\n",
      "\n",
      "9220300298 10224:Int.Coll:01-09-2022 to 30-09-\n",
      "\n",
      "30-09-2022 2022 15158.00 -1778687.10| 555\n",
      "NEFT/P284220196350035/ADITYA\n",
      "\n",
      "11-10-2022 ENTERPRISES/FundsTra 50000.00 -1728687.10| 248\n",
      "NEFT/P294220198735111/ADITYA\n",
      "\n",
      "21-10-2022 ENTERPRISES/fundtran 250000.00 -1478687.10| 248\n",
      "\n",
      "22-10-2022 GST @18% on Charge 4.50 -1478691.60| 555\n",
      "\n",
      "22-10-2022 Consolidated Charges for A/c 25.00 -1478716.60| 555\n",
      "9220300298 10224: Int.Coll:01-10-2022 to 31-10-\n",
      "\n",
      "31-10-2022 2022 17460.00 -1496176.60| 555\n",
      "IMPS/P2A/2308 18737924//CANARAB/X00163 1/\n",
      "\n",
      "04-11-2022 fund 100005.90 -1596182.50| 555\n",
      "INB/NEFT/AXIC223 148486408/samsung\n",
      "\n",
      "10-11-2022 /advance paymen 75000.00 -1671182.50| 555\n",
      "NEFT/P321220203640390/ADITYA\n",
      "\n",
      "17-11-2022 ENTERPRISES 20000.00 -1651182.50| 248\n",
      "\n",
      "23-11-2022 GST @18% on Charge 4.50 -1651187.00| 555\n",
      "\n",
      "23-11-2022 Consolidated Charges for A/c 25.00 -1651212.00| 555\n",
      "INB/NEFT/AX1C223283672549/samsung india\n",
      "\n",
      "24-11-2022 el/PAYMEN 43990.00 -1695202.00| 555\n",
      "9220300298 10224:Int.Coll:0 1-11-2022 to 30-11-\n",
      "\n",
      "30-11-2022 2022 16845.00 -1712047.00| 555\n",
      "NEFT/P347220208579578/ADITYA\n",
      "\n",
      "13-12-2022 ENTERPRISES/AxisBank 25000.00 -1687047.00| 248\n",
      "\n",
      "16-12-2022 GST @18% on Charge 4.50 -1687051.50| 555\n",
      "\n",
      "16-12-2022 Consolidated Charges for A/c 25.00 -1687076.50| 555\n",
      "INB/NEFT/AX1C223575574294/s/Blance advance\n",
      "\n",
      "23-12-2022 paymen 226000.00 -1913076.50|_ 555\n",
      "9220300298 10224: Int.Coll:01-12-2022 to 31-12-\n",
      "\n",
      "31-12-2022 2022 19004.00 -1932080.50| 555\n",
      "NEFT/P014230214970554/ADITYA\n",
      "\n",
      "14-01-2023 ENTERPRISES/Fundtran 50000.00 -1882080.50| 248\n",
      "\n",
      "14-01-2023 GST @18% on Charge 4.50 -1882085.00| 555\n",
      "\n",
      "14-01-2023 Consolidated Charges for A/c 25.00 -1882110.00| 555\n",
      "9220300298 10224:Int.Coll:01-01-2023 to 31-01-\n",
      "\n",
      "31-01-2023 2023 20769.00 -1902879.00| 555\n",
      "NEFT/P047230221494949/ADITYA\n",
      "\n",
      "16-02-2023 ENTERPRISES /Interest 40000.00 -1862879.00| 248\n",
      "\n",
      "17-02-2023 GST @18% on Charge 4.50 -1862883.50| 555\n",
      "\n",
      "17-02-2023 Consolidated Charges for A/c 25.00 -1862908.50| 555\n",
      "9220300298 10224: Int.Coll:01-02-2023 to 28-02-\n",
      "\n",
      "28-02-2023 2023 18767.00 -1881675.50|_ 555\n",
      "NEFT/P067230226107290/ADITYA\n",
      "\n",
      "08-03-2023 ENTERPRISES/ 40000.00 -1841675.50| 248\n",
      "\n",
      "16-03-2023 Annual Service Fee-FY 23-24 18699.00 -1860374.50| 245\n",
      "\n",
      "17-03-2023 GST @18% on Charge 4.50 -1860379.00| 555\n",
      "\n",
      "17-03-2023 Consolidated Charges for A/c 25.00 -1860404.00| 555\n",
      "9220300298 10224: Int.Coll:0 1-03-2023 to 31-03-\n",
      "\n",
      "31-03-2023 2023 20699.00 -1881103.00| 555\n",
      "NEFT/P101230234016267/ADITYA\n",
      "\n",
      "11-04-2023 ENTERPRISES/ 25000.00 -1856103.00| 248\n",
      "\n",
      "15-04-2023 GST @18% on Charge 4.50 -1856107.50| 555\n",
      "\n",
      "15-04-2023 Consolidated Charges for A/c 25.00 -1856132.50| 555\n",
      "9220300298 10224: Int.Coll:01-04-2023 to 30-04-\n",
      "\n",
      "30-04-2023 2023 20075.00 -1876207.50|_ 555\n",
      "\n",
      "08-05-2023 Renewal Processing Fee 5496.00 -1881703.50| 245\n",
      "NEFT/P132230240789529/ADITYA\n",
      "\n",
      "12-05-2023 ENTERPRISES/ 50000.00 -1831703.50| 248\n",
      "\n",
      "12-05-2023 ICICI LOMBARD-INSURANCE PREMIU 2998.00 -1834701.50| 245\n",
      "\n",
      "20-05-2023 GST @18% on Charge 4.50 -1834706.00| 555\n",
      "\n",
      "20-05-2023 Consolidated Charges for A/c 25.00 -1834731.00| 555\n",
      "\n",
      "\n",
      "\n",
      "==============================\n",
      "   OCR Text - Page 3\n",
      "==============================\n",
      "\n",
      "9220300298 10224: Int.Coll:01-05-2023 to 31-05-\n",
      "31-05-2023 2023 20585.00 -1855316.00| 555\n",
      "NEFT/P164230247532291/ADITYA\n",
      "13-06-2023 ENTERPRISES/ 40000.00 -1815316.00| 248\n",
      "16-06-2023 GST @18% on Charge 4.50 -1815320.50| 555\n",
      "16-06-2023 Consolidated Charges for A/c 25.00 -1815345.50| 555\n",
      "23-06-2023 ICICI LOMBARD-INSURANCE PREMIU 2546.03 -1817891.53| 245\n",
      "TRANSACTION TOTAL 2787382.53| _1012200.00\n",
      "CLOSING BALANCE -1817891.53\n",
      "\n",
      "Charge Statement of Axis Account No :922030029810224 for the period (From : 25-06-2022 To : 25-06-2023)\n",
      "\n",
      "Sr. No. Period Recover Date Charge Type Total(RS). Charges(RS).\n",
      "\n",
      "Monthly Service\n",
      "\n",
      "1 01-2023 2023-02-17 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "2 02-2023 2023-03-17 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "3 03-2023 2023-04-15 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "4 04-2023 2023-05-20 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "5 05-2023 2023-06-16 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "6 06-2022 2022-07-23 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "7 07-2022 2022-08-29 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "8 08-2022 2022-09-17 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "9 09-2022 2022-10-22 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "10 10-2022 2022-11-23 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "11 11-2022 2022-12-16 00:00:00 Charge 25 25\n",
      "Monthly Service\n",
      "\n",
      "12 12-2022 2023-01-14 00:00:00 Charge 25 25\n",
      "\n",
      "1. The 'charges' in the above statement indicate the net chargeable amount for the month. However the actual charge debited to the account\n",
      "might have elements of past unrecoverd charge also.\n",
      "\n",
      "2. The chargeable amount is exclusive of Goods and Serivce Tax.\n",
      "\n",
      "Unless the constituent notifies the bank immediately of any discrepancy found by him/her in this statement of Account, it will be taken that\n",
      "he/she has found the account correct.\n",
      "\n",
      "The closing balance as shown/displayed includes not only the credit balance and / or overdraft limit, but also funds which are under clearing. It\n",
      "excludes the amount marked as lien, if any. Hence the closing balance displayed may not be the effective available balance. For any further\n",
      "clarifications, please contact the Branch.\n",
      "\n",
      "We would like to reiterate that, as a policy, Axis Bank does not ask you to part with/disclose/revalidate of your iConnect passord,login id and\n",
      "debit card number through emails OR phone call Further,we would like to reiterate that Axis Bank shall not be liable for any losses arising\n",
      "from you sharing/disclosing of your login id, password and debit card number to anyone. Please co-operate by forwarding all such\n",
      "suspicious/spam emails, if received by you, to customer.service@axisbank.com\n",
      "\n",
      "With effect from 1st August 2016, the replacement charges for Debit card and ATM card applicable on Current accounts have been revised. To\n",
      "know more about the applicable charges,please visit www.axisbank.com\n",
      "\n",
      "Deposit Insurance and Credit Guarantee Corporation (DICGC) insurance cover is applicable in all Banks' deposits, such as savings, current,\n",
      "fixed, recurring etc* up to maximum amount of Rs 5 Lakh including principal & interest both* (* or exceptions and details please refer\n",
      "www.dicgc.org.in )\n",
      "\n",
      "In compliance with regulatory guidelines, the non-CTS cheque books attached to the accounts would be destroyed in banks core banking\n",
      "System. Thus, Non CTS cheques will not be valid for CASH, Clearing and Transfer transactions\n",
      "\n",
      "REGISTERED OFFICE - AXIS BANK LTD,TRISHUL,Opp. Samartheswar Temple, Near Law Garden, Ellisbridge, Ahmedabad .\n",
      "380006. This is a system generated output and requires no signature.\n",
      "\n",
      "BRANCH ADDRESS - AXIS BANK LTD, SURYARAOPET, SH NO.101 TO 106,, CSI DHANWADA ANANTHAM COMPLEX,,\n",
      "\n",
      "PRAKASAM ROAD , SURYARAO PET, 520002, VJAYAWADA, ANDHRA PRADESH, INDIA, TEL:0866-2577722/2575559\n",
      "FAX:6666643\n",
      "\n",
      "\n",
      "\n",
      "==============================\n",
      "   OCR Text - Page 4\n",
      "==============================\n",
      "\n",
      "Legends :\n",
      "\n",
      "ICONN-Transaction trough Internet Banking\n",
      "\n",
      "VMT-ICON-Visa Money Transfer through Internet Banking\n",
      "AUTOSWEEP-Transfer to linked fixed deposit\n",
      "\n",
      "REV SWEEP-Interest on Linked fixed Deposit\n",
      "\n",
      "SWEEP TRF-Transfer from Linked Fixed Deposit / Account\n",
      "VMT-Visa Money Transfer through ATM\n",
      "\n",
      "CWDR-Cash Withdrawal through ATM\n",
      "\n",
      "PUR-POS purchase\n",
      "\n",
      "TIP/ SCG-Surcharge on usage of debit card at pumps/railway ticket purchase or hotel tips\n",
      "RATE.DIFF-Difference in rates on usage of card internationally\n",
      "CLG-Cheque Clearing Transaction\n",
      "\n",
      "EDC-Credit transaction through EDC Machine\n",
      "\n",
      "SETU -Seamless electronic fund transfer through AXIS Bank\n",
      "Int.pd-Interest paid to customer\n",
      "\n",
      "Int.Coll-Interest collected from the customer\n",
      "\n",
      "++++ End of Statement ++++\n",
      "Request From: 10.9.113.229\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_bin = r'D:\\Mridul.Intern\\poppler-23.05.0\\Library\\bin'\n",
    "\n",
    "pdf_path = \"data/old pdfs/axis_aditya.pdf\"\n",
    "pages = convert_from_path(pdf_path, 300, poppler_path=poppler_bin)\n",
    "\n",
    "for i, page in enumerate(pages):\n",
    "    text = pytesseract.image_to_string(page)\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"   OCR Text - Page {i+1}\")\n",
    "    print(f\"{'='*30}\\n\")\n",
    "    print(text.strip())\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90303f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kotak Mahindra Bank\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
    "from PIL import Image\n",
    "\n",
    "pdf_path = \"data/old pdfs/kotak_parag.pdf\"\n",
    "poppler_bin = r\"D:\\\\Mridul.Intern\\\\poppler-23.05.0\\\\Library\\\\bin\"\n",
    "\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-docvqa-large\")\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-docvqa-large\")\n",
    "\n",
    "# pdf to image\n",
    "pages = convert_from_path(pdf_path, dpi=200, poppler_path=poppler_bin)\n",
    "page = pages[0].convert(\"RGB\")\n",
    "width, height = page.size\n",
    "top_half = page.crop((0, 0, width, height // 2))\n",
    "top_half.save(\"pdf-img.png\")\n",
    "\n",
    "question = \"Get the name of the account holder, usually near the address and is an organisation or a person's name. If not found, return 'Not found'\"\n",
    "\n",
    "inputs = processor(images=top_half, text=question, return_tensors=\"pt\")\n",
    "predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "answer = processor.batch_decode(predictions, skip_special_tokens=True)[0]\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9dfa942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted from text - Account No: 0891201001631, Account Holder: ADITYA ENTERPRISES\n",
      "Processed retry\\canara_aditya.xlsx, (956) rows\n",
      "Extracted from text - Account No: 567821400006, Account Holder: LABHANSHI MULTITRADE PVT LTD\n",
      "Processed retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_01.06.2023 to 30.11.2023.xlsx, (918) rows\n",
      "Extracted from text - Account No: 567821400006, Account Holder: LABHANSHI MULTITRADE PVT LTD\n",
      "Processed retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_01.12.2022 to 31.05.2023.xlsx, (801) rows\n",
      "Extracted from text - Account No: 567821400006, Account Holder: LABHANSHI MULTITRADE PVT LTD\n",
      "No valid tables found on page 101, trying OCR\n",
      "Processed retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_Canara CA Jan'23 to Jul'23.xlsx, (850) rows\n",
      "Extracted from text - Account No: 567821400006, Account Holder: LABHANSHI MULTITRADE PVT LTD\n",
      "No valid tables found on page 88, trying OCR\n",
      "Processed retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_Canara(006)-Jan To June 2023.xlsx, (739) rows\n",
      "Extracted from text - Account No: 567821400006, Account Holder: LABHANSHI MULTITRADE PVT LTD\n",
      "Processed retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_Canara(006)-Jan to March 2024.xlsx, (518) rows\n",
      "Extracted from text - Account No: 567821400006, Account Holder: LABHANSHI MULTITRADE PVT LTD\n",
      "Processed retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_Canara(006)-July To Dec 2023.xlsx, (1004) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 APR 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 JAN 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 JUNE 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510000000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 MAR 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 MAY 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 NOV 23.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 OCT 23.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI -781 SEP 23.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_idbi JULY-781.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI-781 AUG 23.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "No valid tables found on page 3, trying OCR\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI-781 AUG 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI-781 DEC 23.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "No valid tables found on page 2, trying OCR\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI-781 FEB 24.xlsx, (0) rows\n",
      "Extracted from text - Account No: 119510200000781, Account Holder: 119510200000781\n",
      "Processed retry\\rmp_farm__idbi__rmp_farm__bank_statement_IDBI-781 JULY 24.xlsx, (0) rows\n",
      "Combined Excel saved to retry\\labhanshi_multitrade_pvt_ltd_canara_ca_labhanshi_multitrade_pvt_ltd_bank_statement_combined.xlsx\n",
      "Combined Excel saved to retry\\rmp_farm__idbi__rmp_farm__bank_statement_combined.xlsx\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# classifications dict\n",
    "CLASSIFICATIONS = {\n",
    "    'ECS/NACH': ['ecs', 'nach', 'ach', 'ift'],\n",
    "    'IMPS': ['imps'],\n",
    "    'NEFT': ['neft'],\n",
    "    'RTGS': ['rtgs'],\n",
    "    'ESIC' : ['esic'],\n",
    "    'Interest' : ['interest'],\n",
    "    'Refund/Reversal' : ['refund', 'reversal', 'rev'],\n",
    "    'Salary': ['salary', 'stipend'],\n",
    "    'Tax': ['tax', 'duty', 'customs'],\n",
    "    'Credit Card' : ['credit card','cc'],\n",
    "    'Debit Card' : ['debit card', 'dc'],\n",
    "    'Bank Instrument' : ['dd', 'commissioner'],\n",
    "    'Cash Txn' : ['cash', 'withdrawal', 'deposit'],\n",
    "    'Cheque Txn' : ['cheque', 'chq', 'clearing', 'clg'],\n",
    "    'Company Expense' : ['expense', 'business', 'corporate', 'travel', \n",
    "                         'home', 'charges', 'employee', 'fund', 'reimbursement', \n",
    "                         'renumeration', 'leave'],\n",
    "    'Forex' : ['forex', 'brn'],\n",
    "    'Insurance' : ['insurance', 'premium'],\n",
    "    'Rent' : ['rent'],\n",
    "    'UPI' : ['upi'],\n",
    "    'Other': []\n",
    "}\n",
    "\n",
    "desc_list = ['description', 'remark', 'transaction', 'detail', 'particulars']\n",
    "\n",
    "def extract_account_info_from_text(text, pdf_path=None, bbox_acc_no=None, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Extract account number and account holder name from PDF text content.\n",
    "    Try Pix2Struct first, then fallback to regex/OCR.\n",
    "    \"\"\"\n",
    "    account_number = None\n",
    "    account_holder = None\n",
    "\n",
    "    # pix2struct try for acc no and name\n",
    "    try:\n",
    "        poppler_bin = r\"D:\\\\Mridul.Intern\\\\poppler-23.05.0\\\\Library\\\\bin\"\n",
    "        model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-docvqa-large\")\n",
    "        processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-docvqa-large\")\n",
    "\n",
    "        pages = convert_from_path(pdf_path, dpi=200, poppler_path=poppler_bin)\n",
    "        page = pages[0].convert(\"RGB\")\n",
    "        width, height = page.size\n",
    "        top_half = page.crop((0, 0, width, height // 2))\n",
    "\n",
    "        prompt_holder = \"Get the name of the account holder. It does not end with 'bank', 'finance' and the like. If not found, return 'Not found'.\"\n",
    "        inputs_holder = processor(images=top_half, text=prompt_holder, return_tensors=\"pt\")\n",
    "        pred_holder = model.generate(**inputs_holder, max_new_tokens=64)\n",
    "        answer_holder = processor.batch_decode(pred_holder, skip_special_tokens=True)[0]\n",
    "        if answer_holder and \"not found\" not in answer_holder.lower():\n",
    "            account_holder = answer_holder.strip()\n",
    "\n",
    "        prompt_accno = \"Get the account number which is usually a long string of integers. If not found, return 'Not found'.\"\n",
    "        inputs_accno = processor(images=top_half, text=prompt_accno, return_tensors=\"pt\")\n",
    "        pred_accno = model.generate(**inputs_accno, max_new_tokens=64)\n",
    "        answer_accno = processor.batch_decode(pred_accno, skip_special_tokens=True)[0]\n",
    "        if answer_accno and \"not found\" not in answer_accno.lower():\n",
    "            account_number = answer_accno.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Pix2Struct extraction failed: {e}\")\n",
    "\n",
    "    if not account_number or not account_holder:\n",
    "        # regex for acc no (allowing for alphanumeric, e.g. SBIN0001234567)\n",
    "        acc_no_patterns = [\n",
    "            r'Account\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "            r'A/c\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "            r'Account\\s*Number\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "            r'Acc\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})'\n",
    "        ]\n",
    "\n",
    "        if not account_number:\n",
    "            for pattern in acc_no_patterns:\n",
    "                match = re.search(pattern, text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    account_number = match.group(1)\n",
    "                    break\n",
    "\n",
    "            # OCR fallback if not found\n",
    "            if not account_number and pdf_path and bbox_acc_no:\n",
    "                ocr_text = ocr_text_from_bbox(pdf_path, bbox_acc_no, poppler_bin, page_num)\n",
    "                for pattern in acc_no_patterns:\n",
    "                    match = re.search(pattern, ocr_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        account_number = match.group(1)\n",
    "                        break\n",
    "                if not account_number:\n",
    "                    generic_match = re.search(r'\\b[A-Z0-9]{6,}\\b', ocr_text)\n",
    "                    if generic_match:\n",
    "                        account_number = generic_match.group(0)\n",
    "\n",
    "        if not account_holder:\n",
    "            lines = text.split('\\n')\n",
    "            for i, line in enumerate(lines[:30]):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                lcline = line.lower()\n",
    "                if any(skip in lcline for skip in ['statement', 'account', 'period', 'bank', 'branch', 'customer', 'number', 'date', 'summary', 'address']):\n",
    "                    continue\n",
    "                if any(char.isdigit() for char in line):\n",
    "                    continue\n",
    "                if re.search(r'[^a-zA-Z\\s\\.]', line):\n",
    "                    continue\n",
    "                words = line.split()\n",
    "                if 1 < len(words) <= 4 and all(w[0].isupper() or w.isupper() for w in words):\n",
    "                    account_holder = line\n",
    "                    break\n",
    "\n",
    "            joint_holder_match = re.search(r'([A-Z\\s]+)\\s*Joint\\s+Holder', text, re.IGNORECASE)\n",
    "            if joint_holder_match and not account_holder:\n",
    "                potential_name = joint_holder_match.group(1).strip()\n",
    "                if len(potential_name) > 3:\n",
    "                    account_holder = potential_name\n",
    "\n",
    "    return account_number, account_holder\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract all text content from PDF for parsing account information.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "def ocr_page_to_dataframe(pdf_path, page_num, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Use OCR to extract tabular data from a PDF page when table detection fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"Using OCR for page {page_num + 1}...\")\n",
    "        images = convert_from_path(pdf_path, dpi=300, poppler_path=poppler_bin, first_page=page_num+1, last_page=page_num+1)\n",
    "        \n",
    "        if not images:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        img = images[0]\n",
    "        \n",
    "        # tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(img, config='--psm 6')\n",
    "        return parse_ocr_text_to_dataframe(text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(f\"OCR failed for page {page_num + 1}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def parse_ocr_text_to_dataframe(text):\n",
    "    \"\"\"\n",
    "    Parse OCR text into a structured dataframe for bank statements.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        if any(skip in line.lower() for skip in ['opening balance', 'closing balance', 'transaction total', 'statement', 'account', 'branch', 'address']):\n",
    "            continue\n",
    "        \n",
    "        date_match = re.search(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{4})\\b', line)\n",
    "        if date_match:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4: # min parts in transaction\n",
    "                try:\n",
    "                    date = date_match.group(1)\n",
    "                    amounts = []\n",
    "                    description_parts = []\n",
    "                    \n",
    "                    for part in parts:\n",
    "                        clean_part = part.replace(',', '').replace('(', '').replace(')', '')\n",
    "                        try:\n",
    "                            amount = float(clean_part)\n",
    "                            amounts.append(amount)\n",
    "                        except ValueError:\n",
    "                            if part != date:\n",
    "                                description_parts.append(part)\n",
    "                    \n",
    "                    if amounts:\n",
    "                        description = ' '.join(description_parts)\n",
    "                        debit = None\n",
    "                        credit = None\n",
    "                        balance = None\n",
    "                        \n",
    "                        if len(amounts) >= 3:\n",
    "                            balance = amounts[-1]  # last amount is usually balance\n",
    "                            if len(amounts) == 3:\n",
    "                                if amounts[0] != 0:\n",
    "                                    debit = amounts[0]\n",
    "                                if amounts[1] != 0:\n",
    "                                    credit = amounts[1]\n",
    "                        elif len(amounts) == 2:\n",
    "                            balance = amounts[-1]\n",
    "                            if 'cr' in line.lower() or 'credit' in line.lower():\n",
    "                                credit = amounts[0]\n",
    "                            else:\n",
    "                                debit = amounts[0]\n",
    "                        elif len(amounts) == 1:\n",
    "                            balance = amounts[0]\n",
    "                        \n",
    "                        rows.append({\n",
    "                            'date': date,\n",
    "                            'description': description,\n",
    "                            'debit': debit,\n",
    "                            'credit': credit,\n",
    "                            'balance': balance\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        # print(f\"OCR extracted {len(df)} potential transactions\")\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_tables(pdf_path, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Extract tables from PDF and return combined dataframe.\n",
    "    Uses OCR as fallback when table detection fails.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    dfs = []\n",
    "    \n",
    "    # print(f\"Processing {len(doc)} pages\")\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        # print(f\"Processing page {page_num + 1}...\")\n",
    "        tables = page.find_tables()\n",
    "        page_has_data = False\n",
    "        \n",
    "        # fitz table detection\n",
    "        for table_num, tbl in enumerate(tables):\n",
    "            try:\n",
    "                df = tbl.to_pandas()\n",
    "                if not df.empty:\n",
    "                    df = df.dropna(how='all')\n",
    "                    string_cols = df.select_dtypes(include=['object']).columns\n",
    "                    if len(string_cols) > 0:\n",
    "                        mask = df[string_cols].astype(str).apply(lambda x: x.str.strip()).replace('', pd.NA).notna().any(axis=1)\n",
    "                        df = df[mask]\n",
    "                    df = df[df.isna().sum(axis=1) <= 2]\n",
    "                    if not df.empty:\n",
    "                        dfs.append(df)\n",
    "                        page_has_data = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing table {table_num + 1} on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # or try OCR as fallback\n",
    "        if not page_has_data:\n",
    "            print(f\"No valid tables found on page {page_num + 1}, trying OCR\")\n",
    "            ocr_df = ocr_page_to_dataframe(pdf_path, page_num, poppler_bin)\n",
    "            if not ocr_df.empty:\n",
    "                ocr_df = ocr_df[ocr_df.isna().sum(axis=1) <= 2]\n",
    "                if not ocr_df.empty:\n",
    "                    dfs.append(ocr_df)\n",
    "                    # print(f\"OCR added {len(ocr_df)} rows from page {page_num + 1}\")\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        # print(f\"Total combined rows: {len(combined_df)}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def ocr_text_from_bbox(pdf_path, bbox, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Crop the page image using bbox and perform OCR on cropped image.\n",
    "    bbox format: (left, upper, right, lower)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300, poppler_path=poppler_bin)\n",
    "        if page_num >= len(images):\n",
    "            return \"\"\n",
    "            \n",
    "        img = images[page_num].crop()\n",
    "        \n",
    "        # set tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(img, config='--psm 6').strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def classify_transaction(description):\n",
    "    \"\"\"\n",
    "    Classify transactions based on keywords in description.\n",
    "    \"\"\"\n",
    "    if pd.isna(description):\n",
    "        return 'Other'\n",
    "        \n",
    "    description_lower = str(description).lower()\n",
    "    \n",
    "    for category, keywords in CLASSIFICATIONS.items():\n",
    "        if any(keyword in description_lower for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "def normalize(df):\n",
    "    \"\"\"\n",
    "    Normalize and clean the extracted dataframe.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        columns = ['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    df.columns = (df.columns\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .str.replace(r'\\s+', '_', regex=True)\n",
    "                    .str.replace(r'\\.', '', regex=True))\n",
    "\n",
    "    # Find the first description-like column present\n",
    "    desc_cols_found = [col for col in desc_list if col in df.columns]\n",
    "    if desc_cols_found:\n",
    "        desc_col = desc_cols_found[0]\n",
    "        df.rename(columns={desc_col: 'description'}, inplace=True)\n",
    "    else:\n",
    "        df['description'] = pd.NA\n",
    "\n",
    "    date_columns = [col for col in df.columns if 'date' in col]\n",
    "    if date_columns:\n",
    "        df['date'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "        df['month_year'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%m-%Y')\n",
    "    else:\n",
    "        df['date'] = None\n",
    "        df['month_year'] = None\n",
    "\n",
    "    df['serial_no'] = range(1, len(df) + 1)\n",
    "\n",
    "    for col in ['debit', 'credit', 'balance']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').replace({'nan': None, 'NaN': None, '': None})\n",
    "            df[col] = df[col].apply(lambda x: pd.to_numeric(x, errors='coerce') if x is not None else pd.NA)\n",
    "        else:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # classification\n",
    "    df['classification'] = df['description'].apply(classify_transaction)\n",
    "    df['account_holders_name'] = None\n",
    "\n",
    "    return df[['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']]\n",
    "\n",
    "def process_statement(input_pdf, output_file, bbox_acc_no=None, bbox_acc_name=None, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process bank statement PDF and extract structured data.\n",
    "    Uses text extraction as primary method, OCR as fallback.\n",
    "    \"\"\"\n",
    "    # print(f\"Processing {input_pdf}\")\n",
    "    \n",
    "    # use fitz\n",
    "    pdf_text = extract_text_from_pdf(input_pdf)\n",
    "    acc_no, acc_name = extract_account_info_from_text(pdf_text, pdf_path=input_pdf, bbox_acc_no=bbox_acc_no, poppler_bin=poppler_bin)\n",
    "    print(f\"Extracted from text - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    # or fallback to OCR\n",
    "    if (not acc_no or not acc_name) and bbox_acc_no and bbox_acc_name:\n",
    "        print(\"Falling back to OCR method\")\n",
    "        if not acc_no:\n",
    "            acc_no = ocr_text_from_bbox(input_pdf, bbox_acc_no, poppler_bin)\n",
    "        if not acc_name:\n",
    "            acc_name = ocr_text_from_bbox(input_pdf, bbox_acc_name, poppler_bin)\n",
    "        # print(f\"OCR results - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    df_raw = extract_tables(input_pdf, poppler_bin)\n",
    "    # print(df_raw.tail())\n",
    "    # print(f\"Extracted {len(df_raw)} raw rows from tables\")\n",
    "    \n",
    "    df = normalize(df_raw)\n",
    "\n",
    "    # Filter out rows with more than 2 missing values\n",
    "    df = df[df.isna().sum(axis=1) <= 2]\n",
    "\n",
    "    if acc_name:\n",
    "        df['account_holders_name'] = acc_name\n",
    "    if acc_no:\n",
    "        df['acc_no'] = acc_no\n",
    "    \n",
    "    # save to Excel\n",
    "    df.to_excel(output_file, index=False)\n",
    "    # print(f\"Saved {len(df)} processed rows to {output_file}\")\n",
    "    \n",
    "    return df, acc_no, acc_name\n",
    "\n",
    "def process_folder(input_folder, output_folder, bbox_acc_no=None, bbox_acc_name=None, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process all PDFs in input_folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "    # print(f\"Found {len(pdf_files)} PDF files in {input_folder}\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        input_pdf = os.path.join(input_folder, pdf_file)\n",
    "        output_file = os.path.join(output_folder, os.path.splitext(pdf_file)[0] + \".xlsx\")\n",
    "        # print(f\"\\n Processing {input_pdf}\")\n",
    "        try:\n",
    "            df, account_number, account_holder = process_statement(input_pdf, output_file, bbox_acc_no, bbox_acc_name, poppler_bin)\n",
    "            print(f\"Processed {output_file}, ({len(df)}) rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_pdf}: {e}\")\n",
    "            continue\n",
    "\n",
    "def longest_common_prefix(strings):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    s1 = min(strings)\n",
    "    s2 = max(strings)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1\n",
    "\n",
    "def combine_excels_if_similar(output_folder, threshold=80):\n",
    "    \"\"\"\n",
    "    Combine Excel files in output_folder into a single Excel file if their filenames are at least\n",
    "    `threshold` percent similar (fuzzy match). Do not save individuals if combined.\n",
    "    \"\"\"\n",
    "    excel_files = [f for f in os.listdir(output_folder) if f.lower().endswith('.xlsx')]\n",
    "    if not excel_files:\n",
    "        # print(\"No Excel files found to combine.\")\n",
    "        return\n",
    "\n",
    "    groups = []\n",
    "    used = set()\n",
    "    for i, file1 in enumerate(excel_files):\n",
    "        if file1 in used:\n",
    "            continue\n",
    "        group = [file1]\n",
    "        used.add(file1)\n",
    "        for file2 in excel_files[i+1:]:\n",
    "            if file2 in used:\n",
    "                continue\n",
    "            score = fuzz.ratio(os.path.splitext(file1)[0], os.path.splitext(file2)[0])\n",
    "            if score >= threshold:\n",
    "                group.append(file2)\n",
    "                used.add(file2)\n",
    "        groups.append(group)\n",
    "\n",
    "    for group in groups:\n",
    "        if len(group) > 1:\n",
    "            # print(f\"Combining files: {group}\")\n",
    "            dfs = []\n",
    "            for fname in group:\n",
    "                df = pd.read_excel(os.path.join(output_folder, fname))\n",
    "                df['source_file'] = fname\n",
    "                dfs.append(df)\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "            # Remove individual files\n",
    "            for fname in group:\n",
    "                os.remove(os.path.join(output_folder, fname))\n",
    "            # Name combined file based on common prefix or joined names\n",
    "            base_names = [os.path.splitext(f)[0] for f in group]\n",
    "            prefix = longest_common_prefix(base_names).rstrip(\"_- \")\n",
    "            if not prefix or len(prefix) < 3:\n",
    "                prefix = \"_\".join(base_names)\n",
    "            combined_path = os.path.join(output_folder, f\"{prefix}_combined.xlsx\")\n",
    "            combined_df.to_excel(combined_path, index=False)\n",
    "            print(f\"Combined Excel saved to {combined_path}\")\n",
    "        # else:\n",
    "            # print(f\"File {group[0]} has no similar files (>= {threshold}%) to combine. Keeping as is.\")\n",
    "\n",
    "# run locally\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"data/pdf\"\n",
    "    output_folder = \"retry\"\n",
    "    poppler_bin = None\n",
    "\n",
    "    process_folder(input_folder, output_folder, poppler_bin)\n",
    "    combine_excels_if_similar(output_folder, threshold=80)      #threshold for fuzzy\n",
    "\n",
    "# ocr run\n",
    "# file_name = \"kotak_parag\"\n",
    "# pdf_path = \"data/old pdfs/kotak_parag.pdf\"\n",
    "# poppler_bin = r\"D:\\\\Mridul.Intern\\\\poppler-23.05.0\\\\Library\\\\bin\"\n",
    "\n",
    "# df = extract_tables(pdf_path, poppler_bin=poppler_bin)\n",
    "# if not df.empty:\n",
    "#     df.to_csv(\"ocr_kotak_parag.csv\", index=False)\n",
    "#     print(\"Saved\")\n",
    "# else:\n",
    "#     print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# classifications dict\n",
    "CLASSIFICATIONS = {\n",
    "    'ECS/NACH': ['ecs', 'nach', 'ach', 'ift'],\n",
    "    'IMPS': ['imps'],\n",
    "    'NEFT': ['neft'],\n",
    "    'RTGS': ['rtgs'],\n",
    "    'ESIC' : ['esic'],\n",
    "    'Interest' : ['interest'],\n",
    "    'Refund/Reversal' : ['refund', 'reversal', 'rev'],\n",
    "    'Salary': ['salary', 'stipend'],\n",
    "    'Tax': ['tax', 'duty', 'customs'],\n",
    "    'Credit Card' : ['credit card','cc'],\n",
    "    'Debit Card' : ['debit card', 'dc'],\n",
    "    'Bank Instrument' : ['dd', 'commissioner'],\n",
    "    'Cash Txn' : ['cash', 'withdrawal', 'deposit'],\n",
    "    'Cheque Txn' : ['cheque', 'chq', 'clearing', 'clg'],\n",
    "    'Company Expense' : ['expense', 'business', 'corporate', 'travel', \n",
    "                         'home', 'charges', 'employee', 'fund', 'reimbursement', \n",
    "                         'renumeration', 'leave'],\n",
    "    'Forex' : ['forex', 'brn'],\n",
    "    'Insurance' : ['insurance', 'premium'],\n",
    "    'Rent' : ['rent'],\n",
    "    'UPI' : ['upi'],\n",
    "    'Other': []\n",
    "}\n",
    "\n",
    "desc_list = ['description', 'remark', 'transaction', 'detail', 'particulars']\n",
    "\n",
    "def extract_account_info_from_text(text, pdf_path=None, bbox_acc_no=None, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Extract account number and account holder name from PDF text content.\n",
    "    If not found, use OCR on the provided bbox to extract account number.\n",
    "    \"\"\"\n",
    "    account_number = None\n",
    "    account_holder = None\n",
    "\n",
    "    # regex for acc no (allowing for alphanumeric, e.g. SBIN0001234567)\n",
    "    acc_no_patterns = [\n",
    "        r'Account\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'A/c\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'Account\\s*Number\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})',\n",
    "        r'Acc\\s*No\\.?\\s*[:\\-]?\\s*([A-Z0-9]{6,})'\n",
    "    ]\n",
    "\n",
    "    for pattern in acc_no_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            account_number = match.group(1)\n",
    "            break\n",
    "\n",
    "    # OCR fallback if not found\n",
    "    if not account_number and pdf_path and bbox_acc_no:\n",
    "        ocr_text = ocr_text_from_bbox(pdf_path, bbox_acc_no, poppler_bin, page_num)\n",
    "        # Try to extract account number from OCR text\n",
    "        for pattern in acc_no_patterns:\n",
    "            match = re.search(pattern, ocr_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                account_number = match.group(1)\n",
    "                break\n",
    "        # If still not found, try to find any long alphanumeric sequence\n",
    "        if not account_number:\n",
    "            generic_match = re.search(r'\\b[A-Z0-9]{6,}\\b', ocr_text)\n",
    "            if generic_match:\n",
    "                account_number = generic_match.group(0)\n",
    "\n",
    "    # Account holder extraction (same as your logic)\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines[:30]):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        lcline = line.lower()\n",
    "        # Skip lines with known non-name keywords or numbers\n",
    "        if any(skip in lcline for skip in ['statement', 'account', 'period', 'bank', 'branch', 'customer', 'number', 'date', 'summary', 'address']):\n",
    "            continue\n",
    "        if any(char.isdigit() for char in line):\n",
    "            continue\n",
    "        # Skip lines with special chars\n",
    "        if re.search(r'[^a-zA-Z\\s\\.]', line):\n",
    "            continue\n",
    "        # Heuristic: 2-4 words, each word capitalized or uppercase, not too long\n",
    "        words = line.split()\n",
    "        if 1 < len(words) <= 4 and all(w[0].isupper() or w.isupper() for w in words):\n",
    "            account_holder = line\n",
    "            break\n",
    "\n",
    "    # look for patterns around \"Joint Holder\" or similar\n",
    "    joint_holder_match = re.search(r'([A-Z\\s]+)\\s*Joint\\s+Holder', text, re.IGNORECASE)\n",
    "    if joint_holder_match and not account_holder:\n",
    "        potential_name = joint_holder_match.group(1).strip()\n",
    "        if len(potential_name) > 3:\n",
    "            account_holder = potential_name\n",
    "\n",
    "    return account_number, account_holder\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract all text content from PDF for parsing account information.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "def ocr_page_to_dataframe(pdf_path, page_num, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Use OCR to extract tabular data from a PDF page when table detection fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(f\"Using OCR for page {page_num + 1}...\")\n",
    "        images = convert_from_path(pdf_path, dpi=300, poppler_path=poppler_bin, first_page=page_num+1, last_page=page_num+1)\n",
    "        \n",
    "        if not images:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        img = images[0]\n",
    "        \n",
    "        # tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(img, config='--psm 6')\n",
    "        return parse_ocr_text_to_dataframe(text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(f\"OCR failed for page {page_num + 1}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def parse_ocr_text_to_dataframe(text):\n",
    "    \"\"\"\n",
    "    Parse OCR text into a structured dataframe for bank statements.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        if any(skip in line.lower() for skip in ['opening balance', 'closing balance', 'transaction total', 'statement', 'account', 'branch', 'address']):\n",
    "            continue\n",
    "        \n",
    "        date_match = re.search(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{4})\\b', line)\n",
    "        if date_match:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4: # min parts in transaction\n",
    "                try:\n",
    "                    date = date_match.group(1)\n",
    "                    amounts = []\n",
    "                    description_parts = []\n",
    "                    \n",
    "                    for part in parts:\n",
    "                        clean_part = part.replace(',', '').replace('(', '').replace(')', '')\n",
    "                        try:\n",
    "                            amount = float(clean_part)\n",
    "                            amounts.append(amount)\n",
    "                        except ValueError:\n",
    "                            if part != date:\n",
    "                                description_parts.append(part)\n",
    "                    \n",
    "                    if amounts:\n",
    "                        description = ' '.join(description_parts)\n",
    "                        debit = None\n",
    "                        credit = None\n",
    "                        balance = None\n",
    "                        \n",
    "                        if len(amounts) >= 3:\n",
    "                            balance = amounts[-1]  # last amount is usually balance\n",
    "                            if len(amounts) == 3:\n",
    "                                if amounts[0] != 0:\n",
    "                                    debit = amounts[0]\n",
    "                                if amounts[1] != 0:\n",
    "                                    credit = amounts[1]\n",
    "                        elif len(amounts) == 2:\n",
    "                            balance = amounts[-1]\n",
    "                            if 'cr' in line.lower() or 'credit' in line.lower():\n",
    "                                credit = amounts[0]\n",
    "                            else:\n",
    "                                debit = amounts[0]\n",
    "                        elif len(amounts) == 1:\n",
    "                            balance = amounts[0]\n",
    "                        \n",
    "                        rows.append({\n",
    "                            'date': date,\n",
    "                            'description': description,\n",
    "                            'debit': debit,\n",
    "                            'credit': credit,\n",
    "                            'balance': balance\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        # print(f\"OCR extracted {len(df)} potential transactions\")\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_tables(pdf_path, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Extract tables from PDF and return combined dataframe.\n",
    "    Uses OCR as fallback when table detection fails.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    dfs = []\n",
    "    \n",
    "    print(f\"Processing {len(doc)} pages\")\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        # print(f\"Processing page {page_num + 1}...\")\n",
    "        tables = page.find_tables()\n",
    "        page_has_data = False\n",
    "        \n",
    "        # fitz table detection\n",
    "        for table_num, tbl in enumerate(tables):\n",
    "            try:\n",
    "                df = tbl.to_pandas()\n",
    "                if not df.empty:\n",
    "                    df = df.dropna(how='all')\n",
    "                    string_cols = df.select_dtypes(include=['object']).columns\n",
    "                    if len(string_cols) > 0:\n",
    "                        mask = df[string_cols].astype(str).apply(lambda x: x.str.strip()).replace('', pd.NA).notna().any(axis=1)\n",
    "                        df = df[mask]\n",
    "                    df = df[df.isna().sum(axis=1) <= 2]\n",
    "                    if not df.empty:\n",
    "                        dfs.append(df)\n",
    "                        page_has_data = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing table {table_num + 1} on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # or try OCR as fallback\n",
    "        if not page_has_data:\n",
    "            print(f\"No valid tables found on page {page_num + 1}, trying OCR\")\n",
    "            ocr_df = ocr_page_to_dataframe(pdf_path, page_num, poppler_bin)\n",
    "            if not ocr_df.empty:\n",
    "                ocr_df = ocr_df[ocr_df.isna().sum(axis=1) <= 2]\n",
    "                if not ocr_df.empty:\n",
    "                    dfs.append(ocr_df)\n",
    "                    print(f\"OCR added {len(ocr_df)} rows from page {page_num + 1}\")\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Total combined rows: {len(combined_df)}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def ocr_text_from_bbox(pdf_path, bbox, poppler_bin=None, page_num=0):\n",
    "    \"\"\"\n",
    "    Crop the page image using bbox and perform OCR on cropped image.\n",
    "    bbox format: (left, upper, right, lower)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300, poppler_path=poppler_bin)\n",
    "        if page_num >= len(images):\n",
    "            return \"\"\n",
    "            \n",
    "        img = images[page_num].crop()\n",
    "        \n",
    "        # set tesseract.exe path\n",
    "        if hasattr(pytesseract, 'pytesseract'):\n",
    "            pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\mridul.intern\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "        \n",
    "        text = pytesseract.image_to_string(img, config='--psm 6').strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def classify_transaction(description):\n",
    "    \"\"\"\n",
    "    Classify transactions based on keywords in description.\n",
    "    \"\"\"\n",
    "    if pd.isna(description):\n",
    "        return 'Other'\n",
    "        \n",
    "    description_lower = str(description).lower()\n",
    "    \n",
    "    for category, keywords in CLASSIFICATIONS.items():\n",
    "        if any(keyword in description_lower for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'\n",
    "\n",
    "def normalize(df):\n",
    "    \"\"\"\n",
    "    Normalize and clean the extracted dataframe.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        columns = ['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    df.columns = (df.columns\n",
    "                    .str.strip()\n",
    "                    .str.lower()\n",
    "                    .str.replace(r'\\s+', '_', regex=True)\n",
    "                    .str.replace(r'\\.', '', regex=True))\n",
    "\n",
    "    # Find the first description-like column present\n",
    "    desc_cols_found = [col for col in desc_list if col in df.columns]\n",
    "    if desc_cols_found:\n",
    "        desc_col = desc_cols_found[0]\n",
    "        df.rename(columns={desc_col: 'description'}, inplace=True)\n",
    "    else:\n",
    "        df['description'] = pd.NA\n",
    "\n",
    "    date_columns = [col for col in df.columns if 'date' in col]\n",
    "    if date_columns:\n",
    "        df['date'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "        df['month_year'] = pd.to_datetime(df[date_columns[0]], dayfirst=True, errors='coerce').dt.strftime('%m-%Y')\n",
    "    else:\n",
    "        df['date'] = None\n",
    "        df['month_year'] = None\n",
    "\n",
    "    df['serial_no'] = range(1, len(df) + 1)\n",
    "\n",
    "    for col in ['debit', 'credit', 'balance']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').replace({'nan': None, 'NaN': None, '': None})\n",
    "            df[col] = df[col].apply(lambda x: pd.to_numeric(x, errors='coerce') if x is not None else pd.NA)\n",
    "        else:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # classification\n",
    "    df['classification'] = df['description'].apply(classify_transaction)\n",
    "    df['account_holders_name'] = None\n",
    "\n",
    "    return df[['serial_no', 'account_holders_name', 'date', 'month_year', 'description', 'debit', 'credit', 'balance', 'classification']]\n",
    "\n",
    "def process_statement(input_pdf, output_file, bbox_acc_no=None, bbox_acc_name=None, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process bank statement PDF and extract structured data.\n",
    "    Uses text extraction as primary method, OCR as fallback.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {input_pdf}\")\n",
    "    \n",
    "    # use fitz\n",
    "    pdf_text = extract_text_from_pdf(input_pdf)\n",
    "    acc_no, acc_name = extract_account_info_from_text(pdf_text, pdf_path=input_pdf, bbox_acc_no=bbox_acc_no, poppler_bin=poppler_bin)\n",
    "    print(f\"Extracted from text - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    # or fallback to OCR\n",
    "    if (not acc_no or not acc_name) and bbox_acc_no and bbox_acc_name:\n",
    "        print(\"Falling back to OCR method\")\n",
    "        if not acc_no:\n",
    "            acc_no = ocr_text_from_bbox(input_pdf, bbox_acc_no, poppler_bin)\n",
    "        if not acc_name:\n",
    "            acc_name = ocr_text_from_bbox(input_pdf, bbox_acc_name, poppler_bin)\n",
    "        # print(f\"OCR results - Account No: {acc_no}, Account Holder: {acc_name}\")\n",
    "    \n",
    "    df_raw = extract_tables(input_pdf, poppler_bin)\n",
    "    print(df_raw.tail())\n",
    "    print(f\"Extracted {len(df_raw)} raw rows from tables\")\n",
    "    \n",
    "    df = normalize(df_raw)\n",
    "\n",
    "    # Filter out rows with more than 2 missing values\n",
    "    df = df[df.isna().sum(axis=1) <= 2]\n",
    "\n",
    "    if acc_name:\n",
    "        df['account_holders_name'] = acc_name\n",
    "    if acc_no:\n",
    "        df['acc_no'] = acc_no\n",
    "    \n",
    "    # save to Excel\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} processed rows to {output_file}\")\n",
    "    \n",
    "    return df, acc_no, acc_name\n",
    "\n",
    "def process_folder(input_folder, output_folder, bbox_acc_no=None, bbox_acc_name=None, poppler_bin=None):\n",
    "    \"\"\"\n",
    "    Process all PDFs in input_folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {input_folder}\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        input_pdf = os.path.join(input_folder, pdf_file)\n",
    "        output_file = os.path.join(output_folder, os.path.splitext(pdf_file)[0] + \".xlsx\")\n",
    "        print(f\"\\n Processing {input_pdf}\")\n",
    "        try:\n",
    "            df, account_number, account_holder = process_statement(input_pdf, output_file, bbox_acc_no, bbox_acc_name, poppler_bin)\n",
    "            print(f\"Processed {output_file}, ({len(df)}) rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_pdf}: {e}\")\n",
    "            continue\n",
    "\n",
    "def longest_common_prefix(strings):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    s1 = min(strings)\n",
    "    s2 = max(strings)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1\n",
    "\n",
    "def combine_excels_if_similar(output_folder, threshold=80):\n",
    "    \"\"\"\n",
    "    Combine Excel files in output_folder into a single Excel file if their filenames are at least\n",
    "    `threshold` percent similar (fuzzy match). Do not save individuals if combined.\n",
    "    \"\"\"\n",
    "    excel_files = [f for f in os.listdir(output_folder) if f.lower().endswith('.xlsx')]\n",
    "    if not excel_files:\n",
    "        print(\"No Excel files found to combine.\")\n",
    "        return\n",
    "\n",
    "    groups = []\n",
    "    used = set()\n",
    "    for i, file1 in enumerate(excel_files):\n",
    "        if file1 in used:\n",
    "            continue\n",
    "        group = [file1]\n",
    "        used.add(file1)\n",
    "        for file2 in excel_files[i+1:]:\n",
    "            if file2 in used:\n",
    "                continue\n",
    "            score = fuzz.ratio(os.path.splitext(file1)[0], os.path.splitext(file2)[0])\n",
    "            if score >= threshold:\n",
    "                group.append(file2)\n",
    "                used.add(file2)\n",
    "        groups.append(group)\n",
    "\n",
    "    for group in groups:\n",
    "        if len(group) > 1:\n",
    "            print(f\"Combining files: {group}\")\n",
    "            dfs = []\n",
    "            for fname in group:\n",
    "                df = pd.read_excel(os.path.join(output_folder, fname))\n",
    "                df['source_file'] = fname\n",
    "                dfs.append(df)\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "            # Remove individual files\n",
    "            for fname in group:\n",
    "                os.remove(os.path.join(output_folder, fname))\n",
    "            # Name combined file based on common prefix or joined names\n",
    "            base_names = [os.path.splitext(f)[0] for f in group]\n",
    "            prefix = longest_common_prefix(base_names).rstrip(\"_- \")\n",
    "            if not prefix or len(prefix) < 3:\n",
    "                prefix = \"_\".join(base_names)\n",
    "            combined_path = os.path.join(output_folder, f\"{prefix}_combined.xlsx\")\n",
    "            combined_df.to_excel(combined_path, index=False)\n",
    "            print(f\"Combined Excel saved to {combined_path}\")\n",
    "        else:\n",
    "            print(f\"File {group[0]} has no similar files (>= {threshold}%) to combine. Keeping as is.\")\n",
    "\n",
    "# run locally\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_folder = \"\"\n",
    "#     output_folder = \"\"\n",
    "#     poppler_bin = None\n",
    "\n",
    "#     process_folder(input_folder, output_folder, poppler_bin)\n",
    "#     combine_excels_if_similar(output_folder, threshold=80)      #threshold for fuzzy\n",
    "\n",
    "# ocr run\n",
    "# file_name = \"kotak_parag\"\n",
    "# pdf_path = \"data/old pdfs/kotak_parag.pdf\"\n",
    "# poppler_bin = r\"D:\\\\Mridul.Intern\\\\poppler-23.05.0\\\\Library\\\\bin\"\n",
    "\n",
    "# df = extract_tables(pdf_path, poppler_bin=poppler_bin)\n",
    "# if not df.empty:\n",
    "#     df.to_csv(\"ocr_kotak_parag.csv\", index=False)\n",
    "#     print(\"Saved\")\n",
    "# else:\n",
    "#     print(\"No data extracted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
